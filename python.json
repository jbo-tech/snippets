{
  "multicollinearity_vif_function": {
    "prefix": "multicollinearity_vif_function",
    "body": [
      "import pandas as pd",
      "from sklearn.linear_model import LinearRegression",
      "",
      "def calculate_vif(df, features):",
      "    vif, tolerance = {}, {}",
      "",
      "    for feature in features:",
      "        # Extraire toutes les autres features pour la régression",
      "        X = df[[f for f in features if f != feature]]",
      "        y = df[feature]",
      "",
      "        # Calculer R^2 à partir du modèle de régression linéaire",
      "        r2 = LinearRegression().fit(X, y).score(X, y)",
      "",
      "        # Calculer la tolérance",
      "        tolerance[feature] = 1 - r2",
      "",
      "        # Calculer le VIF",
      "        vif[feature] = 1 / tolerance[feature]",
      "",
      "    # Retourner un DataFrame avec les VIF et les tolérances",
      "    return pd.DataFrame({'VIF': vif, 'Tolerance': tolerance})",
      "",
      "# Exemple d'utilisation avec un DataFrame df et une liste de features",
      "# df = pd.DataFrame(...)  # Remplacez par votre DataFrame réel",
      "# features = ['feature1', 'feature2', 'feature3']  # Remplacez par vos features réelles",
      "",
      "# Calculer les VIF pour les features spécifiées",
      "# vif_df = calculate_vif(df, features)",
      "# print(vif_df)",
      ""
    ]
  },
  "update_table_sqlite": {
    "prefix": "update_table",
    "body": [
      "cursor.execute(\"\"\"",
      "UPDATE ${1:table_name} SET ${2:column1} = ? WHERE ${3:column2} = ?;",
      "\"\"\", (${4:new_value}, ${5:condition_value}))",
      "conn.commit()",
      ""
    ],
    "description": "Updates data in a table in SQLite database."
  },
  "unique_values_in_column": {
    "prefix": "unique_values",
    "body": [
      "unique_values = df['your_column'].unique()",
      "unique_values = df['your_column'].nunique()",
      ""
    ],
    "description": "Finds unique values in a DataFrame column."
  },
  "preprocess_balancing_downsampling_with_tomek_link": {
    "prefix": "preprocess_balancing_downsampling_with_tomek_link",
    "body": [
      "from imblearn.under_sampling import TomekLinks",
      "",
      "tl = TomekLinks()",
      "X_train_res, y_train_res = tl.fit_resample(X_train_preproc, y_train)",
      "",
      "print(X_train_res.shape)",
      "print(y_train_res.shape)",
      "y_train_res.value_counts()",
      ""
    ],
    "description": "Uses Tomek Links for undersampling by removing pairs of closely opposing instances from the majority and minority classes."
  },
  "preprocess_balancing_downsampling_with_random_under_sampler": {
    "prefix": "preprocess_balancing_downsampling_with_random_under_sampler",
    "body": [
      "from imblearn.under_sampling import RandomUnderSampler",
      "",
      "rus = RandomUnderSampler(random_state=42)",
      "X_train_res, y_train_res = rus.fit_resample(X_train_preproc, y_train)",
      "",
      "print(X_train_res.shape)",
      "print(y_train_res.shape)",
      "y_train_res.value_counts()",
      ""
    ],
    "description": "Utilizes Random Under Sampling to balance the dataset by reducing the number of instances in the majority class."
  },
  "ml_svm_linear": {
    "prefix": "ml_svm_linear",
    "body": [
      "from sklearn import datasets",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.svm import SVC",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score",
      "",
      "# Chargement des données d'exemple",
      "iris = datasets.load_iris()",
      "X = iris.data",
      "y = iris.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Instancier le modèle SVM linéaire",
      "model = SVC(",
      "    kernel='linear',",
      "    C=1.0, # Regularization Paramete : élevé -> overfitting",
      "    tol=1e-3, # Tolerance for stopping criterion",
      "    max_iter=1000 # Maximum number of iterations",
      ")",
      "",
      "# Ajuster le modèle",
      "model.fit(X_train, y_train)",
      "",
      "# Prédire les valeurs de test",
      "y_pred = model.predict(X_test)",
      "",
      "# Évaluer le modèle",
      "accuracy = accuracy_score(y_test, y_pred)",
      "precision = precision_score(y_test, y_pred, average='macro')",
      "recall = recall_score(y_test, y_pred, average='macro')",
      "f1 = f1_score(y_test, y_pred, average='macro')",
      "",
      "print(f'Accuracy: {accuracy}')",
      "print(f'Precision: {precision}')",
      "print(f'Recall: {recall}')",
      "print(f'F1 Score: {f1}')",
      ""
    ],
    "description": "Sets up a Support Vector Machine classifier, ideal for both linear and non-linear classification."
  },
  "ml_svm_nonlinear": {
    "prefix": "ml_svm_nonlinear",
    "body": [
      "from sklearn import datasets",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.svm import SVC",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score",
      "",
      "# Chargement des données d'exemple",
      "iris = datasets.load_iris()",
      "X = iris.data",
      "y = iris.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Instancier le modèle SVM linéaire",
      "model = SVC(",
      "    kernel='rbf', # Radial Basis Function ou 'poly', 'sigmoid'",
      "    gamma='scale', # Influence d'un seul exemple d'entraînement",
      "    C=1.0, # Regularization Paramete : élevé -> overfitting",
      "    tol=1e-3, # Tolerance for stopping criterion",
      "    max_iter=1000 # Maximum number of iterations",
      ")",
      "",
      "# Ajuster le modèle",
      "model.fit(X_train, y_train)",
      "",
      "# Prédire les valeurs de test",
      "y_pred = model.predict(X_test)",
      "",
      "# Évaluer le modèle",
      "accuracy = accuracy_score(y_test, y_pred)",
      "precision = precision_score(y_test, y_pred, average='macro')",
      "recall = recall_score(y_test, y_pred, average='macro')",
      "f1 = f1_score(y_test, y_pred, average='macro')",
      "",
      "print(f'Accuracy: {accuracy}')",
      "print(f'Precision: {precision}')",
      "print(f'Recall: {recall}')",
      "print(f'F1 Score: {f1}')",
      ""
    ],
    "description": "Sets up a Support Vector Machine classifier, ideal for both linear and non-linear classification."
  },
  "facetgrid_with_seaborn": {
    "prefix": "facetgrid_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('tips')",
      "# Create a grid",
      "g = sns.FacetGrid(data, col='sex', row='time', margin_titles=True)",
      "# Plot a graph in each grid element",
      "g.map(sns.scatterplot, 'total_bill', 'tip')",
      "# Global config",
      "g.set_axis_labels('Total Bill', 'Tip')",
      "g.set_titles(col_template='{col_name} day', row_template='{row_name}')",
      "g.add_legend()",
      "plt.show()"
    ],
    "description": "Subplots avec Seaborn FacetGrid pour afficher des scatter plots de Total Bill vs Tip, séparés par sexe et par moment de la journée"
  },
  "random_forest_classifier": {
    "prefix": "random_forest_classifier",
    "body": [
      "from sklearn.ensemble import RandomForestClassifier",
      "",
      "rf_model = RandomForestClassifier(random_state=42)",
      "rf_model.fit(X_train, y_train)",
      "y_pred = rf_model.predict(X_test)",
      ""
    ],
    "description": "Creates a Random Forest Classifier, a robust and commonly used machine learning algorithm for classification tasks."
  },
  "precision_recall_curve": {
    "prefix": "precision_recall_curve_simple",
    "body": [
      "from sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve",
      "",
      "y_pred_prob = pipe.predict_proba(X_test)[:,1]",
      "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)",
      "disp = PrecisionRecallDisplay(precision=precision, recall=recall)",
      "disp.plot()",
      ""
    ],
    "description": "Generates a precision vs recall curve, ideal for assessing performance on imbalanced datasets."
  },
  "pca_reduction": {
    "prefix": "pca_reduction",
    "body": [
      "from sklearn.decomposition import PCA",
      "",
      "pca = PCA(n_components=2)",
      "X_pca = pca.fit_transform(X)",
      "print(X_pca)",
      ""
    ],
    "description": "Reduces the dimensionality of the data using Principal Component Analysis (PCA) to enhance model performance and reduce computational cost."
  },
  "preprocess_balancing_oversampling_with_smote": {
    "prefix": "preprocess_balancing_oversampling_with_smote",
    "body": [
      "import pandas as pd",
      "from imblearn.over_sampling import SMOTE",
      "from sklearn.model_selection import train_test_split",
      "",
      "# Exemple de DataFrame",
      "data = {",
      "    'default': [0, 0, 0, 0, 1, 1],",
      "    'feature1': [10, 20, 30, 40, 50, 60],",
      "    'feature2': [15, 25, 35, 45, 55, 65]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Séparer les features et la target",
      "X = df.drop('default', axis=1)",
      "y = df['default']",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
      "",
      "# Appliquer SMOTE pour l'oversampling",
      "smote = SMOTE(random_state=42)",
      "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)",
      "",
      "# Afficher la distribution des classes avant et après oversampling",
      "print('Distribution des classes avant oversampling:')",
      "print(y_train.value_counts())",
      "print('Distribution des classes après oversampling:')",
      "print(y_train_resampled.value_counts())",
      ""
    ],
    "description": "Applies SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset by increasing the number of instances in the minority class."
  },
  "preprocess_balancing_oversampling_manual": {
    "prefix": "preprocess_balancing_oversampling_manual",
    "body": [
      "import pandas as pd",
      "",
      "# Séparer les classes 0 et 1",
      "df_class_0 = df[df['default'] == 0.0]",
      "df_class_1 = df[df['default'] == 1.0]",
      "",
      "# Sur-échantillonner la classe 1 pour correspondre au nombre de samples de la classe 0",
      "df_class_1_oversampled = df_class_1.sample(n=len(df_class_0), replace=True, random_state=42)",
      "",
      "# Combiner la classe 0 avec la classe 1 sur-échantillonnée",
      "df_balanced = pd.concat([df_class_0, df_class_1_oversampled])",
      "",
      "# Mélanger le DataFrame (optionnel mais recommandé)",
      "df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)",
      "",
      "print(df['default'].value_counts())"
    ],
    "description": "Utilisation de l'oversampling pour équilibrer les classes dans un dataset"
  },
  "nlp_preprocessing_sentence": {
    "prefix": "nlp_preprocessing_sentence",
    "body": [
      "import string",
      "from nltk import word_tokenize",
      "from nltk.stem import WordNetLemmatizer",
      "from nltk.corpus import stopwords",
      "",
      "",
      "def preprocessing(sentence):",
      "",
      "    stop_words = set(stopwords.words('english')) # you can also choose other languages",
      "",
      "    def lemmatizer_v(sentence) -> list:",
      "        verb_lemmatized = [",
      "            WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs",
      "            for word in sentence",
      "        ]",
      "        return verb_lemmatized",
      "",
      "    def lemmatizer_n(sentence) -> list:",
      "        noun_lemmatized = [",
      "            WordNetLemmatizer().lemmatize(word, pos = \"n\") # v --> nouns",
      "            for word in sentence",
      "        ]",
      "        return noun_lemmatized",
      "",
      "    # Remove whitespace",
      "    sentence = sentence.strip()",
      "    # Lowercase characters",
      "    sentence = sentence.lower()",
      "    # Remove numbers",
      "    sentence = ''.join(char for char in sentence if not char.isdigit())",
      "    # Remove punctuation",
      "    for punctuation in string.punctuation:",
      "        sentence = sentence.replace(punctuation, '')",
      "    # Remove stopwords",
      "    sentence = ' '.join(w for w in sentence.split(' ') if not w in stop_words)",
      "    # Tokenize",
      "    sentence_list = word_tokenize(sentence)",
      "    # Lemmatize",
      "    sentence_list = lemmatizer_n(lemmatizer_v(sentence_list))",
      "",
      "    return ' '.join(sentence_list)",
      ""
    ],
    "description": "A function that will clean a sentence"
  },
  "neural_model_initialize_api_multi_output": {
    "prefix": "neural_model_initialize_api_multi_output",
    "body": [
      "def initialize_model_api_multi_output(alpha=0.01):",
      "    l1_reg = regularizers.l1(l1=alpha)",
      "",
      "    inputs = Input(shape=X_train_preproc.shape[-1])",
      "",
      "    dense1 = layers.Dense(16, activation='relu', kernel_regularizer=l1_reg)(inputs)",
      "    dense2 = layers.Dense(16, activation='relu')(dense1)",
      "    dropout1 = layers.Dropout(0.2)(dense2)",
      "",
      "    output_age = layers.Dense(1, activation='linear', name='output_age')(dropout1)",
      "    output_adoption = layers.Dense(1, activation='sigmoid', name='output_adoption')(dropout1)",
      "",
      "    model = Model(inputs=inputs, outputs=[output_age, output_adoption], name='pet_model')",
      "",
      "    model.compile(optimizer='adam',",
      "                 loss={'output_age': 'mse',",
      "                      'output_adoption': 'binary_crossentropy'},",
      "                 metrics={'output_age': 'mae',",
      "                         'output_adoption': 'accuracy'})",
      "    return model",
      "",
      "model = initialize_model_api_multi_output()",
      "model.summary()",
      ""
    ],
    "description": "Initialize a model with multi-output and the API"
  },
  "neural_model_initialize_api": {
    "prefix": "neural_model_initialize_api",
    "body": [
      "def initialize_model_api():",
      "",
      "    #############################",
      "    #  1 - Model architecture   #",
      "    #############################",
      "",
      "    inputs = Input(shape=X_train_preproc.shape[-1])",
      "",
      "    dense1 = layers.Dense(50, activation='relu')(inputs)",
      "    dense2 = layers.Dense(16, activation='relu')(dense1)",
      "    drop = layers.Dropout(0.20)(dense2)",
      "",
      "    outputs = layers.Dense(7, activation='softmax')(drop)",
      "",
      "    model = Model(inputs=inputs, outputs=outputs, name='pet_model')",
      "",
      "    #############################",
      "    #  2 - Optimization Method  #",
      "    #############################",
      "    my_adam = optimizers.Adam(learning_rate = 0.001) # default",
      "    model.compile(loss='categorical_crossentropy', # different from binary_crossentropy because we have multiple classes",
      "                    optimizer=my_adam,",
      "                    metrics=[",
      "                        metrics.MeanSquaredError(name='my_mse'),",
      "                        metrics.AUC(name='my_auc'),",
      "                    ]",
      "    )",
      "",
      "    return model",
      "",
      "model = initialize_model_api()",
      "model.summary()",
      ""
    ],
    "description": "Create a function which initializes a Dense Neural network"
  },
  "neural_model_initialize": {
    "prefix": "neural_model_initialize",
    "body": [
      "from tensorflow.keras import Sequential, layers, metrics",
      "",
      "def initialize_model(input_dim=X_train_preproc.shape[-1]):",
      "",
      "    #############################",
      "    #  1 - Model architecture   #",
      "    #############################",
      "",
      "    # Basically, it will look like a sequence of layers",
      "    model = Sequential()",
      "",
      "    # First layer",
      "    model.add(layers.Dense(50, activation='relu', input_dim=input_dim))",
      "    model.add(layers.Dense(16, activation='relu'))",
      "    model.add(layers.Dropout(0.20))",
      "    model.add(layers.Dense(7, activation='softmax'))",
      "",
      "    #############################",
      "    #  2 - Optimization Method  #",
      "    #############################",
      "    my_adam = optimizers.Adam(learning_rate = 0.001) # default",
      "    model.compile(loss='categorical_crossentropy', # different from binary_crossentropy because we have multiple classes",
      "                    optimizer=my_adam,",
      "                    metrics=[",
      "                        metrics.MeanSquaredError(name='my_mse'),",
      "                        metrics.AUC(name='my_auc'),",
      "                    ]",
      "    )",
      "",
      "    return model",
      "",
      "model = initialize_model()",
      "model.summary()",
      ""
    ],
    "description": "Create a function which initializes a Dense Neural network"
  },
  "neural_model_fit": {
    "prefix": "neural_model_fit",
    "body": [
      "model = initialize_model()",
      "",
      "#############################",
      "#  3 - Model Fit            #",
      "#############################",
      "",
      "es = EarlyStopping(",
      "    monitor = 'val_accuracy',",
      "    patience = 5,",
      "    verbose = 0,",
      "    restore_best_weights = True",
      ")",
      "",
      "# Fit the model",
      "history = model.fit(",
      "    X_train_preproc,",
      "    y_train,",
      "    validation_data = (X_val_preproc, y_val),",
      "    # validation_split = 0.20,",
      "    # shuffle=True,",
      "    batch_size = 64,",
      "    epochs = 500,",
      "    callbacks = [es],",
      "    verbose = 0",
      ")",
      "",
      "# Evaluate the model",
      "res = model.evaluate(X_test_preproc, y_test)",
      "print(f\"accuracy = {res[1]:.3f}\")",
      "",
      "# Plot the loss",
      "plt.plot(history.history['loss'], label=\"Train Loss\")",
      "plt.plot(history.history['val_loss'], label=\"Val Loss\")",
      "plt.legend()",
      "plt.grid()",
      ""
    ],
    "description": "Fit a neural network with EarlyStoping."
  },
  "neural_model_accuracy_plot": {
    "prefix": "neural_model_accuracy_plot",
    "body": [
      "def plot_loss_accuracy(history, title=None):",
      "    fig, ax = plt.subplots(1,2, figsize=(20,7))",
      "",
      "    # --- LOSS ---",
      "",
      "    ax[0].plot(history.history['loss'])",
      "    ax[0].plot(history.history['val_loss'])",
      "",
      "    ax[0].set_title('Model loss')",
      "    ax[0].set_ylabel('Loss')",
      "    ax[0].set_xlabel('Epoch')",
      "",
      "    ax[0].set_ylim((0,3))",
      "",
      "    ax[0].legend(['Train', 'Test'], loc='best')",
      "",
      "    ax[0].grid(axis=\"x\",linewidth=0.5)",
      "    ax[0].grid(axis=\"y\",linewidth=0.5)",
      "",
      "    # --- ACCURACY",
      "",
      "    ax[1].plot(history.history['accuracy'])",
      "    ax[1].plot(history.history['val_accuracy'])",
      "",
      "    ax[1].set_title('Model Accuracy')",
      "    ax[1].set_ylabel('Accuracy')",
      "    ax[1].set_xlabel('Epoch')",
      "",
      "    ax[1].legend(['Train', 'Test'], loc='best')",
      "",
      "    ax[1].set_ylim((0,1))",
      "",
      "    ax[1].grid(axis=\"x\",linewidth=0.5)",
      "    ax[1].grid(axis=\"y\",linewidth=0.5)",
      "",
      "    if title:",
      "        fig.suptitle(title)",
      ""
    ],
    "description": "Plot the history of the model vs the epochs."
  },
  "df_merge_with_pandas": {
    "prefix": "df_merge_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrames d'exemple",
      "df1 = pd.DataFrame({",
      "    'key': ['A', 'B', 'C', 'D'],",
      "    'value': [1, 2, 3, 4]",
      "})",
      "",
      "df2 = pd.DataFrame({",
      "    'key': ['B', 'D', 'E', 'F'],",
      "    'value': [5, 6, 7, 8]",
      "})",
      "",
      "# Fusion des DataFrames sur la colonne 'key'",
      "merged_df = pd.merge(df1, df2, on='key', how='inner')",
      "print(merged_df)"
    ],
    "description": "Fusion de DataFrames avec Pandas en utilisant la méthode merge"
  },
  "manhattan_distance_function": {
    "prefix": "manhattan_distance_function",
    "body": [
      "import math",
      "",
      "def manhattan_distance(start_lat: float, start_lon: float, end_lat: float, end_lon: float) -> float:",
      "    \"\"\"",
      "    Calculate the Manhattan distance between in km two points on the earth (specified in decimal degrees).",
      "    \"\"\"",
      "    earth_radius = 6371",
      "",
      "    lat_1_rad, lon_1_rad = math.radians(start_lat), math.radians(start_lon)",
      "    lat_2_rad, lon_2_rad = math.radians(end_lat), math.radians(end_lon)",
      "",
      "    dlon_rad = lon_2_rad - lon_1_rad",
      "    dlat_rad = lat_2_rad - lat_1_rad",
      "",
      "    manhattan_rad = abs(dlon_rad) + abs(dlat_rad)",
      "    manhattan_km = manhattan_rad * earth_radius",
      "",
      "    return manhattan_km",
      "",
      "def manhattan_distance_vectorized(df: pd.DataFrame, start_lat: str, start_lon: str, end_lat: str, end_lon: str) -> dict:",
      "    \"\"\"",
      "    Calculate the Manhattan distance in km between two points on the earth (specified in decimal degrees).",
      "    Vectorized version for pandas df",
      "    \"\"\"",
      "    earth_radius = 6371",
      "",
      "    lat_1_rad, lon_1_rad = np.radians(df[start_lat]), np.radians(df[start_lon])",
      "    lat_2_rad, lon_2_rad = np.radians(df[end_lat]), np.radians(df[end_lon])",
      "",
      "    dlon_rad = lon_2_rad - lon_1_rad",
      "    dlat_rad = lat_2_rad - lat_1_rad",
      "",
      "    manhattan_rad = np.abs(dlon_rad) + np.abs(dlat_rad)",
      "    manhattan_km = manhattan_rad * earth_radius",
      "",
      "    return manhattan_km",
      ""
    ],
    "description": "Function to calculate Manhattan Distance (L1)"
  },
  "preprocess_encoder_label": {
    "prefix": "preprocess_encoder_label",
    "body": [
      "from sklearn.preprocessing import LabelEncoder",
      "",
      "# Label_encoder object knows",
      "# how to understand word labels.",
      "label_encoder = LabelEncoder()",
      "",
      "# Encode labels in column 'species'.",
      "data['target_encoded']= label_encoder.fit_transform(data['target'])",
      "",
      "print(data['target_encoded'].value_counts(normalize=True))",
      "",
      "pd.concat([data['target'],data['target_encoded']],axis=1).drop_duplicates()",
      ""
    ],
    "description": "Simple implementation of a LabelEncoder"
  },
  "kmeans_plot_centroid": {
    "prefix": "kmeans_plot_centroid",
    "body": [
      "import matplotlib.pyplot as plt",
      "",
      "# Create a range of values for k",
      "k_range = range(1, 5)",
      "",
      "# Initialize an empty list to",
      "# store the inertia values for each k",
      "inertia_values = []",
      "",
      "# Fit and plot the data for each k value",
      "for k in k_range:",
      "\tkmeans = KMeans(n_clusters=k, \\",
      "\t\t\t\t\tinit='k-means++', random_state=42)",
      "\ty_kmeans = kmeans.fit_predict(X)",
      "\tinertia_values.append(kmeans.inertia_)",
      "\tplt.scatter(X[:, 0], X[:, 1], c=y_kmeans)",
      "\tplt.scatter(kmeans.cluster_centers_[:, 0],\\",
      "\t\t\t\tkmeans.cluster_centers_[:, 1], \\",
      "\t\t\t\ts=100, c='red')",
      "\tplt.title('K-means clustering (k={})'.format(k))",
      "\tplt.xlabel('Feature 1')",
      "\tplt.ylabel('Feature 2')",
      "\tplt.show()",
      "",
      "# Plot the inertia values for each k",
      "plt.plot(k_range, inertia_values, 'bo-')",
      "plt.title('Elbow Method')",
      "plt.xlabel('Number of clusters (k)')",
      "plt.ylabel('Inertia')",
      "plt.show()",
      ""
    ],
    "description": "Clustered Data Points For Different k Values"
  },
  "kmeans_elbow_method": {
    "prefix": "kmeans_elbow_method",
    "body": [
      "distortions = []",
      "inertias = []",
      "mapping1 = {}",
      "mapping2 = {}",
      "K = range(1, 10)",
      "",
      "for k in K:",
      "\t# Building and fitting the model",
      "\tkmeanModel = KMeans(n_clusters=k).fit(X)",
      "\tkmeanModel.fit(X)",
      "",
      "\tdistortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,",
      "\t\t\t\t\t\t\t\t\t\t'euclidean'), axis=1)) / X.shape[0])",
      "\tinertias.append(kmeanModel.inertia_)",
      "",
      "\tmapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,",
      "\t\t\t\t\t\t\t\t'euclidean'), axis=1)) / X.shape[0]",
      "\tmapping2[k] = kmeanModel.inertia_",
      "",
      "",
      "plt.plot(K, distortions, 'bx-')",
      "plt.plot(K, inertias, 'ro-')",
      "plt.xlabel('Values of K')",
      "plt.ylabel('Score')",
      "plt.title('The Elbow Method using Distortion')",
      "plt.legend(['Distortion','Inertia'])",
      "plt.show()",
      ""
    ],
    "description": "Now For determining the best number of clusters(k) we plot a graph of k versus their WCSS value."
  },
  "k_means_clustering": {
    "prefix": "k_means_clustering",
    "body": [
      "from sklearn.cluster import KMeans",
      "",
      "kmeans = KMeans(n_clusters=3, random_state=42)",
      "kmeans.fit(X)",
      "labels = kmeans.labels_",
      "print(labels)",
      ""
    ],
    "description": "Applies K-Means clustering, a popular unsupervised learning algorithm to identify clusters within the data."
  },
  "interact_graph": {
    "prefix": "interact_graph",
    "body": [
      "from ipywidgets import interact",
      "import ipywidgets as widgets",
      "",
      "# %matplotlib widget # enable interactivity in your notebook",
      "",
      "@interact(C=[0.1, 1, 10, 100, 1000, 10000], gamma = [0.001, 0.01, 0.1, 1, 10])",
      "def svc(C=1, gamma=1):",
      "    svm = SVC(kernel='rbf', gamma=gamma, C=C)",
      "    svm.fit(X, y)",
      "    plot_decision_regions(X, y, classifier=svm)",
      ""
    ],
    "description": "The interact function automatically creates user interface (UI) controls for exploring code and data interactively."
  },
  "ml_tuning_gridsearch_cv": {
    "prefix": "ml_tuning_gridsearch_cv",
    "body": [
      "from sklearn.model_selection import GridSearchCV",
      "from sklearn.linear_model import ElasticNet",
      "from sklearn.model_selection import train_test_split",
      "",
      "# Train/Test split",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)",
      "",
      "# Instantiate model",
      "model = ElasticNet()",
      "",
      "# Set parameters to search",
      "param_grid = {",
      "    'alpha': [0.01, 0.1, 1],",
      "    'l1_ratio': [0.2, 0.5, 0.8]",
      "}",
      "",
      "# Perform grid search",
      "grid_search = GridSearchCV(",
      "    estimator=model,",
      "    param_grid=param_grid,",
      "    cv=5,",
      "    scoring='accuracy',",
      "    n_jobs=-1 # parallelize computation",
      ")",
      "grid_search.fit(X_train, y_train)",
      "",
      "# Afficher les meilleurs paramètres",
      "print('Meilleurs paramètres:', grid_search.best_params_)",
      "print('Meilleur score:', grid_search.best_score_)",
      "print('Meilleur estimator:', grid_search.best_estimator_)",
      ""
    ],
    "description": "Performs hyperparameter tuning to determine the best parameters for the model."
  },
  "forecast_accuracy_time_series": {
    "prefix": "forecast_accuracy_time_series",
    "body": [
      "import numpy as np",
      "from statsmodels.tsa.stattools import acf",
      "",
      "def forecast_accuracy(y_pred: pd.Series, y_true: pd.Series) -> float:",
      "",
      "    mape = np.mean(np.abs(y_pred - y_true)/np.abs(y_true))  # Mean Absolute Percentage Error",
      "    me = np.mean(y_pred - y_true)             # ME",
      "    mae = np.mean(np.abs(y_pred - y_true))    # MAE",
      "    mpe = np.mean((y_pred - y_true)/y_true)   # MPE",
      "    rmse = np.mean((y_pred - y_true)**2)**.5  # RMSE",
      "    corr = np.corrcoef(y_pred, y_true)[0,1]   # Correlation between the Actual and the Forecast",
      "    mins = np.amin(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)",
      "    maxs = np.amax(np.hstack([y_pred.values.reshape(-1,1), y_true.values.reshape(-1,1)]), axis=1)",
      "    minmax = 1 - np.mean(mins/maxs)             # minmax",
      "    acf1 = acf(y_pred-y_true, fft=False)[1]                      # Lag 1 Autocorrelation of Error",
      "",
      "    forecast = ({",
      "        'mape':mape,",
      "        'me':me,",
      "        'mae': mae,",
      "        'mpe': mpe,",
      "        'rmse':rmse,",
      "        'acf1':acf1,",
      "        'corr':corr,",
      "        'minmax':minmax",
      "    })",
      "",
      "    return forecast",
      ""
    ],
    "description": "Performance metrics : the most common performance metrics for time series"
  },
  "filter_data": {
    "prefix": "filter_data",
    "body": [
      "filtered_df = df[df['your_column'] > some_value]",
      ""
    ],
    "description": "Filters the DataFrame based on a condition applied to columns."
  },
  "feature_importance_extraction": {
    "prefix": "feature_importance",
    "body": [
      "importances = model.feature_importances_",
      "indices = np.argsort(importances)[::-1]",
      "for i in indices:",
      "    print(f'{X.columns[i]}: {importances[i]}')",
      ""
    ],
    "description": "Extracts feature importances from ensemble classifiers to understand the significance of different features."
  },
  "eda_mini_with_plot": {
    "prefix": "eda_mini_with_plot",
    "body": [
      "######################################",
      "# Stats                              #",
      "######################################",
      "from scipy import stats",
      "from statsmodels.graphics.gofplots import qqplot",
      "",
      "######################################",
      "# Data Visualisation                 #",
      "######################################",
      "import matplotlib.pyplot as plt",
      "import seaborn as sns",
      "",
      "for numerical_feature in list(set(numerical_features)-set(cyclical_features)):",
      "",
      "    # Creating three subplots per numerical_feature",
      "    fig, ax =plt.subplots(1,3,figsize=(15,3))",
      "",
      "    # Histogram to get an overview of the distribution of each numerical_feature",
      "    ax[0].set_title(f\"Distribution of: {numerical_feature}\")",
      "    sns.histplot(data = X_train[numerical_features], x = numerical_feature, kde=True, ax = ax[0], color=\"green\");",
      "",
      "    # Boxplot to detect outliers",
      "    ax[1].set_title(f\"Boxplot of: {numerical_feature}\")",
      "    sns.boxplot(data = X_train[numerical_features], x = numerical_feature, ax=ax[1], color=\"green\");",
      "",
      "    # Analyzing whether a feature is normally distributed or not",
      "    ax[2].set_title(f\"Gaussianity of: {numerical_feature}\")",
      "    qqplot(X_train[numerical_features][numerical_feature],line='s',ax=ax[2], c=\"green\");",
      "",
      "    plt.show();",
      ""
    ],
    "description": "Mini Feature EDA with plots"
  },
  "ensemble_learning": {
    "prefix": "ensemble_learning",
    "body": [
      "from sklearn.ensemble import VotingClassifier",
      "",
      "estimators = [('log_reg', log_reg), ('svm', svm_model), ('rf', rf_model)]",
      "ensemble = VotingClassifier(estimators=estimators, voting='hard')",
      "ensemble.fit(X_train, y_train)",
      "y_pred = ensemble.predict(X_test)",
      ""
    ],
    "description": "Implements an ensemble learning technique to improve the predictive performance by combining multiple models."
  },
  "delete_from_table_sqlite": {
    "prefix": "delete_from",
    "body": [
      "cursor.execute(\"\"\"",
      "DELETE FROM ${1:table_name} WHERE ${2:column} = ?;",
      "\"\"\", (${3:value}))",
      "conn.commit()",
      ""
    ],
    "description": "Deletes data from a table in SQLite database based on a condition."
  },
  "decision_tree_classifier": {
    "prefix": "decision_tree_classifier",
    "body": [
      "from sklearn.tree import DecisionTreeClassifier",
      "",
      "dt_model = DecisionTreeClassifier(random_state=42)",
      "dt_model.fit(X_train, y_train)",
      "y_pred = dt_model.predict(X_test)",
      ""
    ],
    "description": "Creates a Decision Tree classifier, a model that learns simple decision rules from the data features."
  },
  "preprocess_scaling_standard_scaler": {
    "prefix": "preprocess_scaling_standard_scaler",
    "body": [
      "import pandas as pd",
      "from sklearn.preprocessing import StandardScaler",
      "",
      "# Exemple de DataFrame",
      "data = pd.DataFrame({",
      "    'A': [1, 2, 3, 4, 5],",
      "    'B': [10, 20, 30, 40, 50]",
      "})",
      "",
      "# Afficher le DataFrame original",
      "print('Original DataFrame:')",
      "print(data)",
      "",
      "# Créer l'objet StandardScaler (Standardizing)",
      "# Normal distribution",
      "# No outliers",
      "scaler = StandardScaler()",
      "",
      "# Appliquer le scaler sur le DataFrame",
      "X_train_scaled = scaler.fit_transform(X_train)",
      "X_test_scaled = scaler.transform(X_test)",
      "",
      "# Afficher le DataFrame après standardisation",
      "print('DataFrame après standardisation:')",
      "print(X_train_scaled)"
    ],
    "description": "Utilisation de StandardScaler pour standardiser les données"
  },
  "preprocess_scaling_minmax": {
    "prefix": "preprocess_scaling_minmax",
    "body": [
      "import pandas as pd",
      "from sklearn.preprocessing import MinMaxScaler",
      "",
      "# Exemple de DataFrame",
      "data = pd.DataFrame({",
      "    'A': [1, 2, 3, 4, 5],",
      "    'B': [10, 20, 30, 40, 50]",
      "})",
      "",
      "# Afficher le DataFrame original",
      "print('Original DataFrame:')",
      "print(data)",
      "",
      "# Créer l'objet MinMaxScaler",
      "# Ensures a fixed range",
      "# Ok for outlier",
      "scaler = MinMaxScaler()",
      "",
      "# Appliquer le scaler sur le DataFrame",
      "data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)",
      "",
      "# Afficher le DataFrame après normalisation",
      "print('DataFrame après normalisation:')",
      "print(data_scaled)"
    ],
    "description": "Utilisation de MinMaxScaler pour normaliser les données"
  },
  "preprocess_scaling_robust": {
    "prefix": "preprocess_scaling_robust",
    "body": [
      "import pandas as pd",
      "from sklearn.preprocessing import RobustScaler",
      "",
      "# Exemple de DataFrame",
      "data = pd.DataFrame({",
      "    'A': [1, 2, 3, 4, 5],",
      "    'B': [10, 20, 30, 40, 50]",
      "})",
      "",
      "# Afficher le DataFrame original",
      "print('Original DataFrame:')",
      "print(data)",
      "",
      "# Créer l'objet RobustScaler",
      "# The median as central tendency",
      "# Ok for outliers",
      "# The interquartile range IQR = Q3 - Q1 as dispersion metric",
      "scaler = RobustScaler()",
      "",
      "# Appliquer le scaler sur le DataFrame",
      "data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)",
      "",
      "# Afficher le DataFrame après scaling robuste",
      "print('DataFrame après scaling robuste:')",
      "print(data_scaled)"
    ],
    "description": "Utilisation de RobustScaler pour mettre à l'échelle les données en utilisant des statistiques de robustesse"
  },
  "cyclical_features_class": {
    "prefix": "cyclical_features_class",
    "body": [
      "class CyclicalFeatures(TransformerMixin, BaseEstimator):",
      "",
      "    def __init__(self):",
      "        pass",
      "",
      "    def fit(self, X, y=None):",
      "        return self",
      "",
      "    def transform(self, X, y=None):",
      "        sin =  np.sin(2 * np.pi * (X.MoSold - 1) / 12)",
      "        cos = np.cos(2 * np.pi * (X.MoSold - 1) / 12)",
      "        self.transformed = pd.DataFrame({'sin_MoSold':sin,",
      "                             'cos_MoSold':cos})",
      "        return self.transformed.reset_index(drop=True)",
      "",
      "    def get_feature_names_out(self):",
      "        return self.columns",
      ""
    ],
    "description": "Transforms cyclical features using cosine and sine functions, allowing models to capture cyclical continuities effectively."
  },
  "ml_cross_validate": {
    "prefix": "ml_cross_validate",
    "body": [
      "from sklearn.model_selection import cross_validate",
      "from sklearn.linear_model import LinearRegression",
      "from sklearn.metrics import make_scorer, mean_squared_error, r2_score",
      "",
      "# Création du modèle",
      "model = LinearRegression()",
      "",
      "# Définition des métriques de scoring",
      "# scoring = {",
      "#     'mse': make_scorer(mean_squared_error),",
      "#     'r2': 'r2'",
      "# }",
      "scoring = [",
      "    'max_error',",
      "    'r2', # Replase scoring",
      "    'neg_mean_absolute_error',",
      "    'neg_mean_squared_error'",
      "]",
      "",
      "# Validation croisée",
      "results = cross_validate(",
      "    model,",
      "    X,",
      "    y,",
      "    cv=5,",
      "    scoring=scoring,",
      "    return_train_score=True,",
      "    n_jobs=-1",
      ")",
      "",
      "# Affichage des résultats",
      "print('Fit Time:', results['fit_time'])",
      "print('Score Time:', results['score_time'])",
      "print('Train MSE:', results['train_mse'])",
      "print('Test MSE:', results['test_mse'])",
      "print('Train R^2:', results['train_r2'])",
      "print('Test R^2:', results['test_r2'])",
      ""
    ],
    "description": "Demonstrates how to perform cross-validation to ensure the model's effectiveness across different subsets of data."
  },
  "ml_cross_validation_learning_curves": {
    "prefix": "ml_cross_validation_learning_curves",
    "body": [
      "from sklearn.model_selection import cross_val_score, learning_curve",
      "from sklearn.linear_model import LinearRegression",
      "",
      "# Validation croisée",
      "model = LinearRegression()",
      "scores = cross_val_score(",
      "    model,",
      "    X_train_scaled,",
      "    y_train,",
      "    cv=5,",
      "    n_jobs=-1",
      ")",
      "print('Cross-Validation Scores:', scores)",
      "print('Mean CV Score:', scores.mean())",
      "",
      "# Courbes d'apprentissage",
      "train_sizes, train_scores, test_scores = learning_curve(model, X_train_scaled, y_train, cv=5)",
      "",
      "# Calculer les moyennes et les écarts-types",
      "train_scores_mean = np.mean(train_scores, axis=1)",
      "train_scores_std = np.std(train_scores, axis=1)",
      "test_scores_mean = np.mean(test_scores, axis=1)",
      "test_scores_std = np.std(test_scores, axis=1)",
      "",
      "# Plot les courbes d'apprentissage",
      "plt.figure()",
      "plt.title('Learning Curves')",
      "plt.xlabel('Training examples')",
      "plt.ylabel('Score')",
      "plt.grid()",
      "",
      "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, \\",
      "                 train_scores_mean + train_scores_std, alpha=0.1, color='r')",
      "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, \\",
      "                 test_scores_mean + test_scores_std, alpha=0.1, color='g')",
      "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')",
      "plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')",
      "",
      "plt.legend(loc='best')",
      "plt.show()",
      ""
    ],
    "description": "Validation croisée et courbes d'apprentissage avec Scikit-Learn"
  },
  "create_table_sqlite": {
    "prefix": "create_table",
    "body": [
      "cursor.execute(\"\"\"",
      "CREATE TABLE IF NOT EXISTS ${1:table_name} (",
      "    ${2:column1_name} ${3:column1_type} PRIMARY KEY,",
      "    ${4:column2_name} ${5:column2_type}",
      ");",
      "\"\"\")",
      ""
    ],
    "description": "Creates a table in SQLite database."
  },
  "ml_classifier_evaluate_calibration_curve": {
    "prefix": "ml_classifier_evaluate_calibration_curve",
    "body": [
      "import numpy as np",
      "import matplotlib.pyplot as plt",
      "from sklearn.datasets import make_classification",
      "from sklearn.linear_model import LogisticRegression",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.calibration import calibration_curve",
      "",
      "# Générer un jeu de données de classification",
      "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)",
      "",
      "# Diviser le jeu de données en ensemble d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Entraîner un modèle de régression logistique",
      "model = LogisticRegression()",
      "model.fit(X_train, y_train)",
      "",
      "# Prédire les probabilités sur l'ensemble de test",
      "y_prob = model.predict_proba(X_test)[:, 1]",
      "",
      "# Calculer la courbe de calibration",
      "prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)",
      "",
      "# Afficher la courbe de calibration",
      "plt.figure(figsize=(10, 6))",
      "plt.plot(prob_pred, prob_true, marker='o', label='Logistic Regression')",
      "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')",
      "plt.xlabel('Mean predicted probability')",
      "plt.ylabel('Fraction of positives')",
      "plt.title('Calibration Curve')",
      "plt.legend()",
      "plt.show()",
      ""
    ],
    "description": "Plots a calibration curve to assess the calibration of predicted probabilities in classification models."
  },
  "ml_time_arima_gridsearch": {
    "prefix": "ml_time_arima_gridsearch",
    "body": [
      "import pmdarima as pm",
      "",
      "model = pm.auto_arima(",
      "    train,",
      "    start_p=0, max_p=3,",
      "    start_q=0, max_q=3,",
      "    d=None,           # let model determine 'd'",
      "    test='adf',       # using adf test to find optimal 'd'",
      "    trace=True,",
      "    error_action='ignore',",
      "    suppress_warnings=True",
      ")",
      "",
      "print(model.summary())",
      ""
    ],
    "description": "Try to run a Grid Search for (p,d,q) using `pmdarima`"
  },
  "module_create": {
    "prefix": "module_create",
    "description": "Creat a well structure module",
    "body": [
      "# mkdir project && cd project",
      "# touch app.py",
      "# mkdir helpers && touch helpers/__init__.py",
      "",
      "# helpers/__init__.py",
      "import random",
      "import string",
      "",
      "def generate_password(size, upper=False):",
      "    \"\"\"Generate a random lowercase ASCII password of given size\"\"\"",
      "    letters = string.ascii_uppercase if upper else string.ascii_lowercase",
      "    breakpoint()",
      "    return ''.join(random.choice(letters) for i in range(size))",
      "",
      "# app.py",
      "from helpers import generate_password # Module import here",
      "",
      "print(generate_password(16))",
      ""
    ]
  },
  "csv_read_with_csv": {
    "prefix": "csv_read_with_csv",
    "body": [
      "import csv",
      "",
      "try:",
      "    with open('data/addresses.csv') as csvfile:",
      "        reader = csv.reader(csvfile, skipinitialspace=True) # reader = csv.DictReader(csvfile)",
      "        for row in reader:",
      "            # row is a `list`",
      "            print(row)",
      "except FileNotFoundError as fnf_error:",
      "    print(f\"File not found error: {fnf_error}\")  # Erreur fichier non trouvé",
      "except PermissionError as perm_error:",
      "    print(f\"Permission error: {perm_error}\")  # Erreur de permission",
      "except csv.Error as csv_error:",
      "    print(f\"CSV error: {csv_error}\")  # Erreur spécifique au module csv",
      "except Exception as e:",
      "    print(f\"An error occurred: {e}\")  # Toute autre erreur",
      ""
    ]
  },
  "csv_write_with_csv": {
    "prefix": "csv_write_with_csv",
    "body": [
      "import csv",
      "",
      "with open('data/beatles.csv', 'w') as csvfile:",
      "    writer = csv.DictWriter(csvfile, fieldnames=beatles[0].keys())",
      "    writer.writeheader()",
      "    for beatle in beatles:",
      "          writer.writerow(beatle)",
      ""
    ]
  },
  "csv_read_with_pandas": {
    "prefix": "csv_read_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "df = pd.read_csv('${1:path/to/your/file.csv}', encoding='utf-8' ${2:, index_col='Id'})",
      ""
    ],
    "description": "Loads data from a CSV file into a pandas DataFrame."
  },
  "csv_write_with_pandas": {
    "prefix": "csv_write_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "df.to_csv('path/to/your/newfile.csv', index=False)",
      ""
    ],
    "description": "Saves a pandas DataFrame to a CSV file."
  },
  "api_get": {
    "prefix": "api_get",
    "body": [
      "import requests",
      "import urllib.parse",
      "",
      "isbn = '0-7475-3269-9'",
      "key = f'ISBN:{isbn}'",
      "BASE_URI = 'https://openlibrary.org'",
      "",
      "url = urllib.parse.urljoin(BASE_URI, \"/api/books\")",
      "",
      "headers = {",
      "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:126.0) Gecko/20100101 Firefox/126.0\",",
      "    \"Accept-Language\":\"en-US\"",
      "}",
      "params = {",
      "    'bibkeys': key,",
      "    'format':'json',",
      "    'jscmd':'data'",
      "}",
      "",
      "try:",
      "    response = requests.get(",
      "        url=url,",
      "        params=params,",
      "        headers=headers",
      "    )",
      "    if response.status_code != 200:",
      "        return 'Error!'",
      "    data = response.json()",
      "    print(data[key]['title'])",
      "except requests.exceptions.HTTPError as http_err:",
      "    print(f\"HTTP error occurred: {http_err}\")  # Erreur HTTP spécifique",
      "except requests.exceptions.ConnectionError as conn_err:",
      "    print(f\"Connection error occurred: {conn_err}\")  # Erreur de connexion",
      "except requests.exceptions.Timeout as timeout_err:",
      "    print(f\"Timeout error occurred: {timeout_err}\")  # Erreur de timeout",
      "except requests.exceptions.RequestException as req_err:",
      "    print(f\"An error occurred: {req_err}\")  # Toute autre erreur",
      ""
    ]
  },
  "scrapping_get": {
    "prefix": "scrapping_get",
    "body": [
      "# Convert cURL command to Python Requests : https://sqqihao.github.io/trillworks.html",
      "",
      "import requests",
      "from bs4 import BeautifulSoup",
      "",
      "url = \"https://www.imdb.com/list/ls055386972/\"",
      "",
      "try:",
      "    response = requests.get(url)",
      "    soup = BeautifulSoup(response.content, \"html.parser\")",
      "    # You now can query the `soup` object!",
      "    soup.title.string",
      "    soup.find('h1', style= \"display:none\")",
      "    soup.find_all('a', class_=\"pizza\")",
      "    soup.find(id=\"wagon\")",
      "except requests.exceptions.HTTPError as http_err:",
      "    print(f\"HTTP error occurred: {http_err}\")  # Erreur HTTP spécifique",
      "except requests.exceptions.ConnectionError as conn_err:",
      "    print(f\"Connection error occurred: {conn_err}\")  # Erreur de connexion",
      "except requests.exceptions.Timeout as timeout_err:",
      "    print(f\"Timeout error occurred: {timeout_err}\")  # Erreur de timeout",
      "except requests.exceptions.RequestException as req_err:",
      "    print(f\"An error occurred: {req_err}\")  # Toute autre erreur",
      "except Exception as e:",
      "    print(f\"An error occurred: {e}\")  # Toute autre erreur",
      ""
    ]
  },
  "sql_get": {
    "prefix": "sql_get",
    "body": [
      "import sqlite3",
      "",
      "connection = sqlite3.connect('data/soccer.sqlite')",
      "cursor = connection.cursor()",
      "",
      "cursor.execute('SELECT * FROM users')",
      "rows = cursor.fetchall() # row = c.fetchone()",
      "",
      "first_row = rows[0]",
      "print(first_row['name'])",
      "print(tuple(first_row))",
      "",
      "for row in rows:",
      "    print(row)",
      "",
      "connection.close()",
      ""
    ]
  },
  "sql_safe_substitution": {
    "prefix": "sql_safe_substitution",
    "body": [
      "import sqlite3",
      "",
      "conn = sqlite3.connect('data/exploitable_db.sqlite')",
      "c = conn.cursor()",
      "",
      "def connect_safe(db, username, password):",
      "  query = \"\"\"",
      "    SELECT *",
      "    FROM users",
      "    WHERE users.username LIKE ?",
      "    AND users.password = ?",
      "  \"\"\"",
      "  db.execute(query, (f'%{username}%', password)) # Be careful : always a tuple",
      "  user = db.fetchone()",
      "  if user is None:",
      "    return \"Unauthorized\"",
      "  else:",
      "    return \"Authorized\"",
      "",
      "def top_five_artists(db, genre_name):",
      "    query = \"\"\"",
      "        SELECT",
      "            artists.name,",
      "            COUNT(*) AS track_count",
      "        FROM artists",
      "        JOIN albums ON albums.artist_id = artists.id",
      "        JOIN tracks ON tracks.album_id = albums.id",
      "        JOIN genres ON tracks.genre_id = genres.id",
      "        WHERE genres.name = ?",
      "        GROUP BY artists.name",
      "        ORDER BY track_count DESC",
      "        LIMIT 5",
      "    \"\"\"",
      "    db.execute(query, (genre_name,))",
      "    return db.fetchall()",
      "",
      "# top_five_artists(c,'Rock')",
      ""
    ]
  },
  "class_oop": {
    "prefix": "class_oop",
    "body": [
      "class Shape:",
      "    def __init__(self, color, name):",
      "        self.color = color",
      "        self.name = name",
      "",
      "    def say_name(self):",
      "        return f\"My name is {self.name}.\"",
      "",
      "",
      "class Rectangle(Shape):",
      "    def __init__(self, color, name, width, height):",
      "        super().__init__(color, name)",
      "        self.width = width",
      "        self.height = height",
      "",
      "    def say_name(self):",
      "        return f\"My name is {self.name} and I am a rectangle.\"",
      ""
    ]
  },
  "sql_get_with_date_delta": {
    "prefix": "sql_get_with_date_delta",
    "body": [
      "def average_number_of_days_between_orders(db):",
      "    # return the average number of days between two consecutive orders of the same customer",
      "    query = \"\"\"",
      "        WITH DatedOrders AS (",
      "            SELECT",
      "                CustomerID,",
      "                OrderID,",
      "                OrderDate,",
      "                LAG(OrderDate, 1, 0) OVER (",
      "                    PARTITION BY CustomerID",
      "                    ORDER By OrderDate",
      "                ) PreviousOrderDate",
      "            FROM Orders",
      "        )",
      "        SELECT ROUND(",
      "            AVG(",
      "                JULIANDAY(OrderDate) - JULIANDAY(PreviousOrderDate)",
      "                )",
      "            ) AS delta",
      "        FROM DatedOrders",
      "        WHERE PreviousOrderDate != 0",
      "    \"\"\"",
      "    return int(db.execute(query).fetchone()[0])",
      ""
    ]
  },
  "sql_get_with_pandas": {
    "prefix": "sql_get_with_pandas",
    "body": [
      "import sqlite3",
      "import pandas as pd",
      "",
      "conn = sqlite3.connect(\"data/blog.sqlite\")",
      "",
      "query = \"\"\"",
      "    WITH daily_likes AS (",
      "        SELECT",
      "            created_at,",
      "            COUNT(*) AS daily_like_count",
      "        FROM likes",
      "        GROUP BY created_at",
      "    )",
      "    SELECT",
      "        created_at date,",
      "        SUM(daily_like_count) OVER(ORDER BY created_at) AS cumulative_daily_like_count",
      "    FROM daily_likes;",
      "\"\"\"",
      "",
      "df = pd.read_sql_query(query, conn)",
      ""
    ]
  },
  "eda_starting_methods": {
    "prefix": "eda_starting_methods",
    "body": [
      "###",
      "### EDA starting methods",
      "###",
      "",
      "print(\"\\n### First rows\")",
      "display(df.head(5)",
      "",
      "print(\"\\n### Last rows\")",
      "display(df.tail(5)",
      "",
      "print(\"\\n### Dimensions du DataFrame.\")",
      "display(df.shape)",
      "",
      "print(\"\\n### Types de données de chaque colonne.\")",
      "display(df.dtypes)",
      "",
      "print(\"\\n### Informations sur le DataFrame.\")",
      "display(df.info())",
      "",
      "print(\"\\n### Statistiques descriptives.\")",
      "display(df.describe().T)",
      "",
      "print(\"\\n### Somme des valeurs nulles par colonne.\")",
      "display(df.isnull().sum())",
      ""
    ]
  },
  "df_boolean_indexing": {
    "prefix": "df_boolean_indexing",
    "body": [
      "df[df['Column'] > value]",
      "df[df['Region'].str.contains('AMER')]",
      "df[df['Region'].isin(['WESTERN EUROPE', 'EASTERN EUROPE'])]",
      ""
    ]
  },
  "df_reindexing": {
    "prefix": "df_reindexing",
    "body": [
      "df['Country'] = df['Country'].map(str.strip)",
      "df.set_index('Country', inplace=True)",
      ""
    ]
  },
  "df_sorting": {
    "prefix": "df_sorting",
    "body": [
      "df.sort_values(by='Column_Name', ascending=False)",
      "df.sort_index(ascending=False)",
      ""
    ]
  },
  "df_grouping": {
    "prefix": "df_grouping",
    "body": [
      "grouped_df = df.groupby('Column_Name')",
      "grouped_df['Column_to_aggregate'].sum()",
      ""
    ]
  },
  "gbq_read_with_pandas": {
    "prefix": "gbq_read_with_pandas",
    "body": [
      "import pandas_gbq",
      "",
      "project_id = 'your-project-id'",
      "sql = \"\"\"",
      "SELECT artist_name, COUNT(artist_name) FROM `listenbrainz.listenbrainz.listen`",
      "WHERE listened_at BETWEEN \"2017-01-01\" AND \"2018-01-01\"",
      "GROUP BY artist_name",
      "HAVING COUNT(artist_name) > 1000",
      "ORDER BY COUNT(artist_name) DESC",
      "\"\"\"",
      "df = pandas_gbq.read_gbq(sql, project_id=project_id)",
      ""
    ]
  },
  "df_regex_match": {
    "prefix": "df_regex_match",
    "body": [
      "import re",
      "",
      "pattern = r'\\d{4}-\\d{2}-\\d{2}'",
      "# Find only the rows that have valid birthdays",
      "only_bdays = df[df[\"birthday\"].str.match(pattern)].copy()",
      ""
    ]
  },
  "line_chart_with_matplotlib": {
    "prefix": "line_chart_with_matplotlib",
    "body": [
      "import matplotlib.pyplot as plt",
      "",
      "x = [1, 2, 3, 4, 5]",
      "y = [10, 20, 15, 25, 30]",
      "",
      "plt.plot(x, y, marker='o', linestyle='-', color='b', label='Line')",
      "plt.title('Line Chart')",
      "plt.xlabel('X-axis')",
      "plt.ylabel('Y-axis')",
      "plt.legend()",
      "plt.grid(True)",
      "plt.show()"
    ],
    "description": "Line chart with Matplotlib"
  },
  "line_chart_with_seaborn": {
    "prefix": "line_chart_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('fmri')",
      "sns.lineplot(x='timepoint', y='signal', hue='event', style='region', data=data)",
      "plt.title('Line Chart')",
      "plt.xlabel('Timepoint')",
      "plt.ylabel('Signal')",
      "plt.legend()",
      "plt.grid(True)",
      "plt.show()"
    ],
    "description": "Line chart with Seaborn"
  },
  "line_chart_with_plotly": {
    "prefix": "line_chart_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "",
      "fig = go.Figure()",
      "fig.add_trace(go.Scatter(x=[1, 2, 3, 4, 5], y=[10, 20, 15, 25, 30], mode='lines+markers', name='Line'))",
      "fig.update_layout(title='Line Chart', xaxis_title='X-axis', yaxis_title='Y-axis')",
      "fig.show()"
    ],
    "description": "Line chart with Plotly"
  },
  "bar_chart_with_matplotlib": {
    "prefix": "bar_chart_with_matplotlib",
    "body": [
      "import matplotlib.pyplot as plt",
      "",
      "categories = ['A', 'B', 'C', 'D', 'E']",
      "values = [10, 20, 15, 25, 30]",
      "",
      "plt.bar(categories, values, color='skyblue')",
      "plt.title('Bar Chart')",
      "plt.xlabel('Categories')",
      "plt.ylabel('Values')",
      "plt.show()"
    ],
    "description": "Bar chart with Matplotlib"
  },
  "bar_chart_with_seaborn": {
    "prefix": "bar_chart_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('titanic')",
      "sns.barplot(x='class', y='fare', data=data, palette='viridis')",
      "# sns.countplot(x='class', hue='sex', data=data);",
      "plt.title('Bar Chart')",
      "plt.xlabel('Class')",
      "plt.ylabel('Fare')",
      "plt.show()",
      ""
    ],
    "description": "Bar chart with Seaborn"
  },
  "bar_chart_with_plotly": {
    "prefix": "bar_chart_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "",
      "fig = go.Figure([go.Bar(x=['A', 'B', 'C', 'D', 'E'], y=[10, 20, 15, 25, 30])])",
      "fig.update_layout(title='Bar Chart', xaxis_title='Categories', yaxis_title='Values')",
      "fig.show()"
    ],
    "description": "Bar chart with Plotly"
  },
  "histogram_with_matplotlib": {
    "prefix": "histogram_with_matplotlib",
    "body": [
      "import matplotlib.pyplot as plt",
      "import numpy as np",
      "",
      "data = np.random.randn(1000)",
      "",
      "plt.hist(data, bins=30, color='purple', cumulative=False, density=True)",
      "plt.title('Histogram')",
      "plt.xlabel('Values')",
      "plt.ylabel('Frequency')",
      "plt.show()",
      ""
    ],
    "description": "Histogram with Matplotlib"
  },
  "histogram_with_seaborn": {
    "prefix": "histogram_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('iris')",
      "sns.histplot(data['sepal_length'], bins=30, color='purple', kde=True)",
      "plt.title('Histogram')",
      "plt.xlabel('Sepal Length')",
      "plt.ylabel('Frequency')",
      "plt.show()"
    ],
    "description": "Histogram with Seaborn"
  },
  "histogram_with_plotly": {
    "prefix": "histogram_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "import numpy as np",
      "",
      "data = np.random.randn(1000)",
      "",
      "fig = go.Figure(data=[go.Histogram(x=data, nbinsx=30)])",
      "fig.update_layout(title='Histogram', xaxis_title='Values', yaxis_title='Frequency')",
      "fig.show()"
    ],
    "description": "Histogram with Plotly"
  },
  "pie_chart_with_matplotlib": {
    "prefix": "pie_chart_with_matplotlib",
    "body": [
      "import matplotlib.pyplot as plt",
      "",
      "sizes = [15, 30, 45, 10]",
      "labels = ['A', 'B', 'C', 'D']",
      "colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']",
      "explode = (0.1, 0, 0, 0)",
      "",
      "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)",
      "plt.title('Pie Chart')",
      "plt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.",
      "plt.show()",
      ""
    ],
    "description": "Pie chart with Matplotlib"
  },
  "pie_chart_with_plotly": {
    "prefix": "pie_chart_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "",
      "labels = ['A', 'B', 'C', 'D']",
      "values = [15, 30, 45, 10]",
      "",
      "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])",
      "fig.update_layout(title='Pie Chart')",
      "fig.show()"
    ],
    "description": "Pie chart with Plotly"
  },
  "scatter_plot_with_matplotlib": {
    "prefix": "scatter_plot_with_matplotlib",
    "body": [
      "import matplotlib.pyplot as plt",
      "",
      "x = [1, 2, 3, 4, 5]",
      "y = [10, 20, 15, 25, 30]",
      "",
      "plt.scatter(x, y, color='red', edgecolor='#333333', alpha=0.75)",
      "plt.title('Scatter Plot')",
      "plt.xlabel('X-axis')",
      "plt.ylabel('Y-axis')",
      "plt.grid(True)",
      "plt.show()",
      ""
    ],
    "description": "Scatter plot with Matplotlib"
  },
  "scatter_plot_with_seaborn": {
    "prefix": "scatter_plot_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('iris')",
      "sns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=data)",
      "plt.title('Scatter Plot')",
      "plt.xlabel('Sepal Length')",
      "plt.ylabel('Sepal Width')",
      "plt.legend()",
      "plt.grid(True)",
      "plt.show()"
    ],
    "description": "Scatter plot with Seaborn"
  },
  "scatter_plot_with_plotly": {
    "prefix": "scatter_plot_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "",
      "fig = go.Figure(data=go.Scatter(x=[1, 2, 3, 4, 5], y=[10, 20, 15, 25, 30], mode='markers'))",
      "fig.update_layout(title='Scatter Plot', xaxis_title='X-axis', yaxis_title='Y-axis')",
      "fig.show()"
    ],
    "description": "Scatter plot with Plotly"
  },
  "heatmap_with_seaborn": {
    "prefix": "heatmap_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "import numpy as np",
      "",
      "#data = np.random.rand(10, 12)",
      "correlation_matrix = df.select_dtypes('number').corr()",
      "column_names = correlation_matrix.columns",
      "sns.heatmap(correlation_matrix, annot=True, xticklabels=column_names, yticklabels=column_names,cmap= \"coolwarm\")",
      "plt.title('Heatmap')",
      "plt.show();",
      ""
    ],
    "description": "Heatmap with Seaborn"
  },
  "heatmap_with_plotly": {
    "prefix": "heatmap_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "import numpy as np",
      "",
      "data = np.random.rand(10, 12)",
      "",
      "fig = go.Figure(data=go.Heatmap(z=data, colorscale='Viridis'))",
      "fig.update_layout(title='Heatmap')",
      "fig.show()"
    ],
    "description": "Heatmap avec Plotly"
  },
  "3d_plot_with_matplotlib": {
    "prefix": "3d-plot_with_matplotlib",
    "body": [
      "import matplotlib.pyplot as plt",
      "from mpl_toolkits.mplot3d import Axes3D",
      "import numpy as np",
      "",
      "fig = plt.figure()",
      "ax = fig.add_subplot(111, projection='3d')",
      "",
      "x = np.linspace(-5, 5, 100)",
      "y = np.linspace(-5, 5, 100)",
      "x, y = np.meshgrid(x, y)",
      "z = np.sin(np.sqrt(x**2 + y**2))",
      "",
      "ax.plot_surface(x, y, z, cmap='viridis')",
      "plt.title('3D Surface Plot')",
      "plt.show()"
    ],
    "description": "Graphique 3D avec Matplotlib"
  },
  "3d_plot_with_plotly": {
    "prefix": "3d-plot_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "import numpy as np",
      "",
      "x = np.linspace(-5, 5, 100)",
      "y = np.linspace(-5, 5, 100)",
      "x, y = np.meshgrid(x, y)",
      "z = np.sin(np.sqrt(x**2 + y**2))",
      "",
      "fig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])",
      "fig.update_layout(title='3D Surface Plot')",
      "fig.show()"
    ],
    "description": "Graphique 3D avec Plotly"
  },
  "boxplot_with_matplotlib": {
    "prefix": "boxplot_with_matplotlib",
    "body": [
      "import matplotlib.pyplot as plt",
      "import numpy as np",
      "",
      "data = np.random.randn(100)",
      "",
      "plt.boxplot(data)",
      "plt.title('Boxplot')",
      "plt.ylabel('Values')",
      "plt.show()"
    ],
    "description": "Boxplot avec Matplotlib"
  },
  "boxplot_with_seaborn": {
    "prefix": "boxplot_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('iris')",
      "sns.boxplot(x='species', y='sepal_length', data=data)",
      "plt.title('Boxplot')",
      "plt.xlabel('Species')",
      "plt.ylabel('Sepal Length')",
      "plt.show()"
    ],
    "description": "Boxplot avec Seaborn"
  },
  "boxplot_with_plotly": {
    "prefix": "boxplot_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "import numpy as np",
      "",
      "data = np.random.randn(100)",
      "",
      "fig = go.Figure(data=[go.Box(y=data)])",
      "fig.update_layout(title='Boxplot', yaxis_title='Values')",
      "fig.show()"
    ],
    "description": "Boxplot avec Plotly"
  },
  "violin_plot_with_seaborn": {
    "prefix": "violin-plot_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('iris')",
      "sns.violinplot(x='species', y='sepal_length', data=data)",
      "plt.title('Violin Plot')",
      "plt.xlabel('Species')",
      "plt.ylabel('Sepal Length')",
      "plt.show()"
    ],
    "description": "Violin plot avec Seaborn"
  },
  "violin_plot_with_plotly": {
    "prefix": "violin-plot_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "import numpy as np",
      "",
      "data = np.random.randn(100)",
      "",
      "fig = go.Figure(data=[go.Violin(y=data)])",
      "fig.update_layout(title='Violin Plot', yaxis_title='Values')",
      "fig.show()"
    ],
    "description": "Violin plot avec Plotly"
  },
  "pairplot_with_seaborn": {
    "prefix": "pairplot_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "# Set the style of the visualization",
      "sns.set_theme(style='whitegrid')",
      "",
      "data = sns.load_dataset('iris')",
      "sns.pairplot(data, hue='species')",
      "plt.show()",
      ""
    ],
    "description": "Pair plot avec Seaborn"
  },
  "subplot_destucturing": {
    "prefix": "subplot_destucturing",
    "body": [
      "import matplotlib.pyplot as plt",
      "",
      "fig, axs = plt.subplots(2, 2, figsize=(10,3))  # 2x2 grid",
      "# First subplot",
      "axs[0, 0].plot(df['x_column'], df['y1_column'])",
      "axs[0, 0].set_title('Subplot 1')",
      "# Second subplot",
      "axs[0, 1].plot(df['x_column'], df['y2_column'])",
      "axs[0, 1].set_title('Subplot 2')",
      "# Third subplot",
      "axs[1, 0].plot(df['x_column'], df['y3_column'])",
      "axs[1, 0].set_title('Subplot 3')",
      "# Fourth subplot",
      "axs[1, 1].plot(df['x_column'], df['y4_column'])",
      "axs[1, 1].set_title('Subplot 4')",
      "# Global figure methods",
      "plt.tight_layout()",
      "plt.suptitle('Global subtitle')",
      "plt.show()",
      ""
    ],
    "description": "Creates subplots using Matplotlib to display multiple plots in a structured grid."
  },
  "subplot_basic": {
    "prefix": "subplot_basic",
    "body": [
      "import matplotlib.pyplot as plt",
      "import numpy as np",
      "",
      "x = np.linspace(0, 10, 100)",
      "",
      "plt.figure(figsize=(10,3))",
      "",
      "# Subplot 1",
      "plt.subplot(2, 1, 1)",
      "plt.plot(x, np.sin(x))",
      "plt.title('Subplot 1: Sine')",
      "",
      "# Subplot 2",
      "plt.subplot(2, 1, 2)",
      "plt.plot(x, np.cos(x))",
      "plt.title('Subplot 2: Cosine')",
      "# Global figure methods",
      "plt.tight_layout()",
      "plt.suptitle('Global subtitle')",
      "plt.show()"
    ],
    "description": "Subplots avec une grille simple utilisant plt.subplot"
  },
  "subplot_object_oriented": {
    "prefix": "subplot_object_oriented",
    "body": [
      "import matplotlib.pyplot as plt",
      "",
      "fig = plt.figure(figsize=(10,3))",
      "# First subplot",
      "ax1 = fig.add_subplot(1,2,1)",
      "ax1.plot(x, y, label=\"coal\")",
      "ax1.plot(x, y, label = \"gas\")",
      "ax1.set_title('coal vs. gas')",
      "ax1.legend()",
      "# Second subplot",
      "ax2 = fig.add_subplot(1,2,2)",
      "ax2.plot(x, y, c='black')",
      "ax2.set_title('all energies')",
      "# Global figure methods",
      "fig.suptitle('US electricity CO2 emissions')",
      "plt.show()",
      ""
    ]
  },
  "subplot_with_plotly": {
    "prefix": "subplot_with_plotly",
    "body": [
      "import plotly.graph_objects as go",
      "from plotly.subplots import make_subplots",
      "",
      "fig = make_subplots(rows=2, cols=2, subplot_titles=('Sine', 'Cosine', 'Tangent', '-Sine'))",
      "",
      "x = [i for i in range(1, 11)]",
      "",
      "fig.add_trace(go.Scatter(x=x, y=[np.sin(i) for i in x]), row=1, col=1)",
      "fig.add_trace(go.Scatter(x=x, y=[np.cos(i) for i in x]), row=1, col=2)",
      "fig.add_trace(go.Scatter(x=x, y=[np.tan(i) for i in x]), row=2, col=1)",
      "fig.add_trace(go.Scatter(x=x, y=[-np.sin(i) for i in x]), row=2, col=2)",
      "",
      "fig.update_layout(height=600, width=800, title_text='Multiple Subplots')",
      "fig.show()"
    ],
    "description": "Subplots avec Plotly et make_subplots"
  },
  "subplot_with_seaborn": {
    "prefix": "subplot_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "import numpy as np",
      "",
      "data = sns.load_dataset('iris')",
      "",
      "fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4, figsize=(10, 8))",
      "",
      "sns.scatterplot(x='sepal_length', y='sepal_width', hue='species', data=data, ax=ax1)",
      "ax1.set_title('Sepal Length vs Sepal Width')",
      "",
      "sns.scatterplot(x='petal_length', y='petal_width', hue='species', data=data, ax=ax2)",
      "ax2.set_title('Petal Length vs Petal Width')",
      "",
      "sns.histplot(data['sepal_length'], kde=True, color='blue', ax=ax3)",
      "ax3.set_title('Sepal Length Distribution')",
      "",
      "sns.boxplot(x='species', y='sepal_width', data=data, ax=ax4)",
      "ax4.set_title('Sepal Width by Species')",
      "",
      "fig.tight_layout()",
      "plt.show()"
    ],
    "description": "Subplots avec Seaborn et plt.subplots"
  },
  "line_chart_with_pandas": {
    "prefix": "line_chart_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "ax = df.plot() # plot all columns against the index",
      "ax.set_title('Title of the chart')  # this is an Axes, thanks to pandas",
      "ax",
      ""
    ]
  },
  "subplot_with_pandas": {
    "prefix": "subplot_with_pandas",
    "body": [
      "df1 = pd.DataFrame({ 'coal': coal_y, 'gas': gas_y }, index=years_x)",
      "df2 = pd.DataFrame({ 'total': total_y }, index=years_x)",
      "",
      "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))",
      "df1.plot(ax=ax1)",
      "df2.plot(ax=ax2)",
      ""
    ]
  },
  "bar_chart_with_pandas": {
    "prefix": "bar_chart_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "df.plot(kind='bar')",
      ""
    ]
  },
  "load_dataset_with_seaborn": {
    "prefix": "load_dataset_with_seaborn",
    "body": [
      "import seaborn as sns",
      "",
      "tips_df = sns.load_dataset('tips')",
      "tips_df.shape",
      ""
    ]
  },
  "catplot_with_seaborn": {
    "prefix": "catplot_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('titanic')",
      "",
      "sns.catplot(x='class', y='fare', hue='sex', kind='box', data=data)",
      "plt.title('Catplot: Box plot of Fare by Class and Sex')",
      "plt.xlabel('Class')",
      "plt.ylabel('Fare')",
      "plt.show()"
    ],
    "description": "Catplot avec Seaborn pour afficher un box plot de Fare par Class et Sex"
  },
  "ml_linear_regression_multivariate_with_seaborn": {
    "prefix": "ml_linear_regression_multivariate_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('tips')",
      "",
      "sns.lmplot(x='total_bill', y='tip', hue='sex', data=data, aspect=1.6, markers=['o', 'x'])",
      "plt.title('Lmplot: Regression of Tip on Total Bill by Sex')",
      "plt.xlabel('Total Bill')",
      "plt.ylabel('Tip')",
      "plt.show()"
    ],
    "description": "Lmplot avec Seaborn pour afficher une régression de Tip sur Total Bill par Sex"
  },
  "ml_linear_regression_with_seaborn": {
    "prefix": "ml_linear_regression_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('tips')",
      "",
      "sns.regplot(x='total_bill', y='tip', data=data, scatter_kws={'color':'red'}, line_kws={'color':'blue'})",
      "plt.title('Regplot: Regression of Tip on Total Bill')",
      "plt.xlabel('Total Bill')",
      "plt.ylabel('Tip')",
      "plt.show()"
    ],
    "description": "Regplot avec Seaborn pour afficher une régression de Tip sur Total Bill"
  },
  "kdeplot_with_seaborn": {
    "prefix": "kdeplot_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('iris')",
      "",
      "sns.kdeplot(data['sepal_length'], data['sepal_width'], shade=True, cmap='Blues')",
      "plt.title('KDE Plot: Sepal Length vs Sepal Width')",
      "plt.xlabel('Sepal Length')",
      "plt.ylabel('Sepal Width')",
      "plt.show()"
    ],
    "description": "KDE plot avec Seaborn pour afficher une estimation de densité du Sepal Length et Sepal Width"
  },
  "jointplot_with_seaborn": {
    "prefix": "jointplot_with_seaborn",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "data = sns.load_dataset('tips')",
      "",
      "sns.jointplot(x='total_bill', y='tip', data=data, kind='reg', color='purple')",
      "plt.title('Joint Plot: Total Bill vs Tip')",
      "plt.show()"
    ],
    "description": "Joint plot avec Seaborn pour afficher une régression entre Total Bill et Tip"
  },
  "contour_with_matplotlib": {
    "prefix": "contour_with_matplotlib",
    "body": [
      "import matplotlib.pyplot as plt",
      "import numpy as np",
      "",
      "x = np.linspace(-5, 5, 100)",
      "y = np.linspace(-5, 5, 100)",
      "X, Y = np.meshgrid(x, y)",
      "Z = np.sin(np.sqrt(X**2 + Y**2))",
      "",
      "plt.contour(X, Y, Z, levels=20, cmap='viridis')",
      "plt.title('Contour Plot')",
      "plt.xlabel('X-axis')",
      "plt.ylabel('Y-axis')",
      "plt.colorbar()",
      "plt.show()"
    ],
    "description": "Contour plot avec Matplotlib pour afficher des iso lines de la fonction sin(sqrt(X^2 + Y^2))"
  },
  "contourf_with_matplotlib": {
    "prefix": "contourf_with_matplotlib",
    "body": [
      "import matplotlib.pyplot as plt",
      "import numpy as np",
      "",
      "x = np.linspace(-5, 5, 100)",
      "y = np.linspace(-5, 5, 100)",
      "X, Y = np.meshgrid(x, y)",
      "Z = np.sin(np.sqrt(X**2 + Y**2))",
      "",
      "plt.contourf(X, Y, Z, levels=20, cmap='viridis')",
      "plt.title('Filled Contour Plot')",
      "plt.xlabel('X-axis')",
      "plt.ylabel('Y-axis')",
      "plt.colorbar()",
      "plt.show()"
    ],
    "description": "Filled contour plot avec Matplotlib pour afficher des zones colorées de la fonction sin(sqrt(X^2 + Y^2))"
  },
  "subplot_with_gridspec": {
    "prefix": "subplot_with_gridspec",
    "body": [
      "import matplotlib.pyplot as plt",
      "import matplotlib.gridspec as gridspec",
      "import numpy as np",
      "",
      "x = np.linspace(0, 10, 100)",
      "y = np.sin(x)",
      "",
      "# Create a GridSpec with 3 rows and 3 columns",
      "fig = plt.figure(figsize=(10, 8))",
      "gs = gridspec.GridSpec(3, 3)",
      "",
      "# Main plot (larger subplot spanning multiple grid cells)",
      "ax1 = fig.add_subplot(gs[0, :])",
      "ax1.plot(x, y)",
      "ax1.set_title('Main Plot: Sine Wave')",
      "",
      "# Subplot 2",
      "ax2 = fig.add_subplot(gs[1, :-1])",
      "ax2.plot(x, np.cos(x), 'r')",
      "ax2.set_title('Subplot 2: Cosine Wave')",
      "",
      "# Subplot 3",
      "ax3 = fig.add_subplot(gs[1:, -1])",
      "ax3.plot(x, np.tan(x), 'g')",
      "ax3.set_title('Subplot 3: Tangent Wave')",
      "",
      "# Subplot 4",
      "ax4 = fig.add_subplot(gs[2, 0])",
      "ax4.plot(x, -y, 'y')",
      "ax4.set_title('Subplot 4: Negative Sine Wave')",
      "",
      "# Subplot 5",
      "ax5 = fig.add_subplot(gs[2, 1])",
      "ax5.plot(x, -np.cos(x), 'c')",
      "ax5.set_title('Subplot 5: Negative Cosine Wave')",
      "",
      "# Global figure methods",
      "fig.suptitle('US electricity CO2 emissions')",
      "fig.tight_layout()",
      "plt.show()",
      ""
    ],
    "description": "Subplots avec Matplotlib et GridSpec pour créer une grille complexe de subplots"
  },
  "df_join_with_pandas": {
    "prefix": "df_join_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrames d'exemple",
      "df1 = pd.DataFrame({",
      "    'key': ['A', 'B', 'C', 'D'],",
      "    'value1': [1, 2, 3, 4]",
      "})",
      "df1.set_index('key', inplace=True)",
      "",
      "df2 = pd.DataFrame({",
      "    'key': ['B', 'D', 'E', 'F'],",
      "    'value2': [5, 6, 7, 8]",
      "})",
      "df2.set_index('key', inplace=True)",
      "",
      "# Jointure des DataFrames sur l'index",
      "joined_df = df1.join(df2, how='inner')",
      "print(joined_df)"
    ],
    "description": "Jointure sur l'index de DataFrames avec Pandas en utilisant la méthode join"
  },
  "df_concat_with_pandas": {
    "prefix": "df_concat_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrames d'exemple",
      "df1 = pd.DataFrame({",
      "    'key': ['A', 'B', 'C', 'D'],",
      "    'value': [1, 2, 3, 4]",
      "})",
      "",
      "df2 = pd.DataFrame({",
      "    'key': ['E', 'F', 'G', 'H'],",
      "    'value': [5, 6, 7, 8]",
      "})",
      "",
      "# Concatenation des DataFrames",
      "concatenated_df = pd.concat([df1, df2], axis=0)",
      "print(concatenated_df)"
    ],
    "description": "Concatenation de DataFrames avec Pandas en utilisant la méthode concat"
  },
  "df_pivot_unstack": {
    "prefix": "df_pivot_unstack",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrame d'exemple avec un index multi-niveau",
      "data = {",
      "    'key1': ['A', 'A', 'B', 'B'],",
      "    'key2': ['C', 'D', 'C', 'D'],",
      "    'value': [1, 2, 3, 4]",
      "}",
      "df = pd.DataFrame(data)",
      "df.set_index(['key1', 'key2'], inplace=True)",
      "",
      "print('DataFrame original:')",
      "print(df)",
      "",
      "# Unstack du DataFrame",
      "unstacked_df = df.unstack()",
      "print('\\nDataFrame unstacked:')",
      "print(unstacked_df)"
    ],
    "description": "Transformation de DataFrame avec unstack de Pandas pour pivoter un index de niveau supérieur"
  },
  "df_groupby_and_operation": {
    "prefix": "df_groupby_and_operation",
    "body": [
      "all_events_df = all_events_df[all_events_df[\"Year\"] >= 1984] \\",
      "    .groupby([\"Country\"]) \\",
      "    .count()[[\"Medal\"]] \\",
      "    .sort_values(by=\"Medal\", ascending=False) \\",
      "    .rename(columns={'Medal':'Event Count'})",
      "top_10_events_df = all_events_df.head(10)",
      "top_10_events_df.plot(kind='bar');",
      ""
    ]
  },
  "json_read_with_pandas": {
    "prefix": "json_read_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Exemple de chaîne JSON",
      "json_data = '''",
      "[",
      "  {\"name\": \"Alice\", \"age\": 25, \"city\": \"New York\"},",
      "  {\"name\": \"Bob\", \"age\": 30, \"city\": \"San Francisco\"},",
      "  {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Los Angeles\"}",
      "]",
      "'''",
      "",
      "# Lecture des données JSON",
      "df = pd.read_json(json_data)",
      "",
      "print('DataFrame lu à partir du JSON:')",
      "print(df)"
    ],
    "description": "Lecture des données JSON avec Pandas en utilisant la méthode read_json"
  },
  "figsize_notebook": {
    "prefix": "figsize_notebook",
    "body": [
      "# Set default figsize for all notebook (usefull in TS analysis)",
      "matplotlib.rcParams['figure.figsize'] = (15, 6)",
      ""
    ]
  },
  "df_pivot_with_pandas": {
    "prefix": "df_pivot_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrame d'exemple",
      "data = {",
      "    'date': ['2021-01-01', '2021-01-01', '2021-01-02', '2021-01-02'],",
      "    'city': ['New York', 'Los Angeles', 'New York', 'Los Angeles'],",
      "    'temperature': [21, 22, 23, 24],",
      "    'humidity': [30, 40, 35, 45]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "print('DataFrame original:')",
      "print(df)",
      "",
      "# Pivot du DataFrame",
      "pivot_df = df.pivot(index='date', columns='city', values='temperature')",
      "# pivot = df.pivot_table(values='value_column', index='index_column', columns='columns_to_pivot', aggfunc='mean')",
      "",
      "print('\\nDataFrame pivoté:')",
      "print(pivot_df)",
      ""
    ],
    "description": "Transformation de DataFrame avec pivot de Pandas pour réorganiser les données"
  },
  "df_aggregate": {
    "prefix": "df_aggregate",
    "body": [
      "import pandas as pd",
      "",
      "df.groupby('city').agg({'city': 'count', 'latitude': 'median', 'longitude': 'median'})",
      ""
    ]
  },
  "df_date_get_year": {
    "prefix": "df_date_get_year",
    "body": [
      "df['dateYear'] = pd.to_datetime(df['dateAdded']).dt.year",
      ""
    ]
  },
  "scatter_geo_with_plotly": {
    "prefix": "scatter_geo_with_plotly_express",
    "body": [
      "import plotly.express as px",
      "",
      "# Données d'exemple",
      "data = {",
      "    'lat': [40.7128, 34.0522, 41.8781, 51.5074],",
      "    'lon': [-74.0060, -118.2437, -87.6298, -0.1278],",
      "    'city': ['New York', 'Los Angeles', 'Chicago', 'London'],",
      "    'population': [8398748, 3990456, 2705994, 8908081]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Création du graphique de dispersion géographique",
      "fig = px.scatter_geo(df.sort_values(by=['dateAdded']),",
      "                     lat='lat',",
      "                     lon='lon',",
      "                     text='city',",
      "                     size='population',",
      "                     size_max=30,",
      "                     color='population',",
      "                     color_continuous_scale='Viridis',",
      "                     title='Scatter Geo Plot')",
      "",
      "fig.update_geos(",
      "    projection_type='natural earth',",
      "    showland=True",
      ")",
      "",
      "fig.show()"
    ],
    "description": "Scatter Geo plot avec Plotly Express pour afficher un graphique de dispersion géographique des villes"
  },
  "plot_like_a_pro": {
    "prefix": "plot_like_a_pro",
    "body": [
      "import seaborn as sns",
      "import matplotlib.pyplot as plt",
      "",
      "with plt.style.color_palette('pastel'):",
      "    colors = ['#73B2AF','#95D697', '#82618C',  '#7F8DAE', '#FCED7C']",
      "    #['Gulf Stream','Granny Smith Apple', 'French lilac', 'Ship Cove', 'Yellow']",
      "    plt.figure(figsize=(20, 10))",
      "",
      "    ax = sns.scatterplot(",
      "        x=\"gdpPercap\",",
      "        y=\"lifeExp\",",
      "        hue=\"continent\",",
      "        data=gdp_df,",
      "        size=\"pop\",",
      "        sizes=(20, 3000),",
      "        palette=sns.color_palette(colors)",
      "    )",
      "",
      "    sns.despine(top=True, bottom=True, left=True, right=True)",
      "",
      "    ax.xaxis.set_label_coords(0.97, -0.05)",
      "    ax.yaxis.set_label_coords(-0.025, 0.93)",
      "    ax.grid(b=True)",
      "    ax.tick_params(axis=u'both', which=u'both',length=0)",
      "    handles, labels = ax.get_legend_handles_labels()",
      "",
      "    ax.legend(handles[-5:], ('250','500','750','1000','1250'), bbox_to_anchor=(0.2, -.15, 1., .102), loc='lower left',",
      "            ncol=5, borderpad=2, frameon=False, handletextpad=2.5, columnspacing=4)",
      "",
      "    ax.annotate(\"Population (M)\", xy=(0.1, -0.11), xycoords='axes fraction', fontsize=14)",
      ""
    ]
  },
  "df_apply_with_pandas": {
    "prefix": "df_apply_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrame d'exemple",
      "data = {",
      "    'A': [1, 2, 3, 4],",
      "    'B': [10, 20, 30, 40]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Appliquer une fonction lambda pour ajouter 10 à chaque valeur",
      "df['A'] = df['A'].apply(lambda x: x + 10)",
      "print('DataFrame après application de apply sur la colonne A:')",
      "print(df)",
      "",
      "# Appliquer une fonction sur l'ensemble des lignes",
      "def add_columns(row):",
      "    return row['A'] + row['B']",
      "",
      "df['C'] = df.apply(add_columns, axis=1)",
      "print('DataFrame après application de apply sur chaque ligne:')",
      "print(df)",
      "",
      "# Appliquer une fonction lambda pour ajouter 10 à chaque élément du DataFrame",
      "df.applymap(lambda x: x + 10)",
      ""
    ],
    "description": "Appliquer une fonction avec apply de Pandas sur une colonne et sur chaque ligne"
  },
  "df_map_with_pandas": {
    "prefix": "df_map_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrame d'exemple",
      "data = {",
      "    'A': [1, 2, 3, 4],",
      "    'B': [10, 20, 30, 40]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Appliquer une fonction lambda pour multiplier chaque valeur par 2 dans la colonne A",
      "df['A'] = df['A'].map(lambda x: x * 2)",
      "print('DataFrame après application de map sur la colonne A:')",
      "print(df)"
    ],
    "description": "Appliquer une fonction avec map de Pandas sur une colonne"
  },
  "preprocess_categorical_with_pandas": {
    "prefix": "preprocess_categorical_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrame d'exemple",
      "data = {",
      "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston'],",
      "    'temperature': [21, 22, 23, 24],",
      "    'humidity': [30, 40, 35, 45]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "print('DataFrame original:')",
      "print(df)",
      "",
      "# Conversion des variables catégorielles en variables indicatrices",
      "df_dummies = pd.get_dummies(df, columns=['city'], prefix='city')",
      "",
      "print('\\nDataFrame avec get_dummies:')",
      "print(df_dummies)"
    ],
    "description": "Conversion des variables catégorielles en variables indicatrices avec get_dummies de Pandas"
  },
  "ml_linear_regression_manual_line_plot_function": {
    "prefix": "ml_linear_regression_manual_line_plot_function",
    "body": [
      "def plot_line(data, slope, intercept):",
      "    # First, we'll create x - an array of our independent variable values - for you.",
      "    x = data['surface']",
      "",
      "    # Now create an array of predicted prices using the predicted prices formula above",
      "    # Remember, we can do mathematical operations on an entire array at once!",
      "    y_pred = None",
      "    # \\$DELETE_BEGIN",
      "    y_pred = x * slope + intercept",
      "    # \\$DELETE_END",
      "",
      "    # Finally, let's plot the line!",
      "    plt.plot(x, y_pred);",
      ""
    ]
  },
  "mse_manual_function": {
    "prefix": "mse_manual_function",
    "body": [
      "def compute_mse(y_true, y_pred):",
      "    \"\"\"",
      "    Calculer l'erreur quadratique moyenne (MSE).",
      "",
      "    Arguments:",
      "    y_true -- Valeurs réelles.",
      "    y_pred -- Valeurs prédites = A * x + B",
      "    A -- slope",
      "    B -- intercept",
      "",
      "    Retourne:",
      "    mse -- Mean Square Error.",
      "    \"\"\"",
      "    mse = np.mean((y_true - y_pred) ** 2)",
      "    return mse",
      ""
    ]
  },
  "ml_linear_regression_ols_with_statsmodels": {
    "prefix": "ml_linear_regression_ols_with_statsmodels",
    "body": [
      "import numpy as np",
      "import pandas as pd",
      "import statsmodels.formula.api as smf",
      "",
      "# Création de DataFrame d'exemple",
      "data = {",
      "    'surface': [30, 40, 50, 60, 70],",
      "    'price': [100, 150, 200, 250, 300]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Ajuster le modèle OLS (Ordinary Least Square)",
      "regression = smf.ols(formula='price ~ surface', data=df).fit()",
      "# regression.summary()",
      "",
      "# Imprimer les paramètres du modèle",
      "print('intercept_best:', regression.params['Intercept'])",
      "print('slope_best:', regression.params['surface'])",
      "",
      "# Calculer et imprimer l'erreur quadratique moyenne (MSE)",
      "mse_best = np.mean(regression.resid**2)",
      "print('mse_best:', mse_best)",
      "",
      "# Calculer la L2 Euclidean Distance",
      "l2_euclidean_distance = np.sqrt(np.sum(regression.resid**2))",
      "print('L2 Euclidean Distance:', l2_euclidean_distance)",
      "",
      "# Calculer le Sum of Squared Residuals (SSR)",
      "ssr = np.sum(regression.resid**2)",
      "print('Sum of Squared Residuals (SSR):', ssr)",
      "",
      "# Calculer le Total Sum of Squares (SST)",
      "sst = np.sum((df['price'] - np.mean(df['price']))**2)",
      "print('Total Sum of Squares (SST):', sst)",
      "",
      "# Calculer de R2 (Goodness-of-fit)",
      "# The model explains a good deal of the observed variance of the dependent variable",
      "rsquared = 1 - (ssr / sst) # regression.rsquared",
      "print('R-squared (R²):', rsquared)",
      ""
    ],
    "description": "Régression linéaire avec OLS de statsmodels.formula.api"
  },
  "library_data_manipulation": {
    "prefix": "library_data_manipulation",
    "body": [
      "######################################",
      "# Data Manipulation                  #",
      "######################################",
      "import numpy as np",
      "import pandas as pd",
      "pd.options.display.max_columns = None",
      ""
    ]
  },
  "library_data_visualisation": {
    "prefix": "library_data_visualisation",
    "body": [
      "######################################",
      "# Data Visualisation                 #",
      "######################################",
      "import matplotlib.pyplot as plt",
      "import seaborn as sns",
      ""
    ]
  },
  "library_statistics": {
    "prefix": "library_statistics",
    "body": [
      "######################################",
      "# Stats                              #",
      "######################################",
      "from scipy import stats",
      "from statsmodels.graphics.gofplots import qqplot",
      ""
    ]
  },
  "df_skewness_with_pandas": {
    "prefix": "df_skewness_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrame d'exemple",
      "data = {",
      "    'values': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 10]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Calcul de la skewness",
      "skewness = df['values'].skew()",
      "print('Skewness:', skewness)"
    ],
    "description": "Calcul de la skewness avec Pandas"
  },
  "df_kurtosis_with_pandas": {
    "prefix": "df_kurtosis_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Création de DataFrame d'exemple",
      "data = {",
      "    'values': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 10]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Calcul de la kurtosis",
      "kurtosis = df['values'].kurtosis()",
      "print('Kurtosis:', kurtosis)"
    ],
    "description": "Calcul de la kurtosis avec Pandas"
  },
  "eda_with_ydata_profiling": {
    "prefix": "eda_with_ydata_profiling",
    "body": [
      "# !pip install ydata-profiling",
      "import pandas as pd",
      "from ydata_profiling import ProfileReport",
      "",
      "# Charger un jeu de données en DataFrame pandas",
      "df = pd.read_csv('votre_fichier.csv')",
      "",
      "# Générer un rapport de profilage",
      "profile = ProfileReport(df, title=\"Profiling Report\", explorative=True)",
      "",
      "# Exporter le rapport en HTML",
      "profile.to_file(\"rapport_profiling.html\")"
    ],
    "description": "Snippet pour générer un rapport de profilage de données avec ydata_profiling"
  },
  "train_test_holdout_methode": {
    "prefix": "train_test_holdout_methode",
    "body": [
      "from sklearn.model_selection import train_test_split",
      "",
      "## 1 - Creating X and y",
      "X = data[['GrLivArea']]",
      "y = data['SalePrice']",
      "",
      "## 2 - Creating train and test assemblies",
      "# Before oversampling and scaling",
      "random_state = 42",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)",
      "",
      "## 3 - Instantiate the model",
      "model = LinearRegression()",
      "",
      "## 4 - Training the model",
      "model.fit(X_train, y_train)",
      "",
      "## 5 - Les coef",
      "print(model.coef_)",
      "print(model.intercept_)",
      "",
      "## 6 - Model evaluation : R2",
      "print(model.score(X_train, y_train))",
      "print(model.score(X_test, y_test))",
      ""
    ]
  },
  "null_hypothesis_with_stats": {
    "prefix": "null_hypothesis_with_stats",
    "body": [
      "# 1. Formulation des  Hypothèses",
      "#",
      "### H0: Le médicament n'a aucun effet",
      "### H1: Le médicament a un effet",
      "",
      "# 2. Choix du Niveau de Signification (α)",
      "#",
      "### α = 0.05",
      "",
      "# 3. Collecte des Données et Calcul des Statistiques",
      "#",
      "### Données échantillonnées",
      "",
      "# 4. Détermination de la Distribution de la Statistique de Test",
      "#",
      "### Distribution normale",
      "",
      "# 5. Calcul de la p-value",
      "",
      "# 6. Prise de Décision",
      "#",
      "### Si p ≤ α : Rejeter H0",
      "### Si p > α : Ne pas rejeter H0",
      "",
      "from scipy.stats import norm",
      "",
      "# Définir la distribution normale",
      "mu = 300",
      "sigma = 50 / (100**0.5)  # équivaut à 5",
      "X = norm(mu, sigma)",
      "",
      "# Calculer la p-value pour une valeur observée de 310",
      "observed_value = 310",
      "p_value = 1 - X.cdf(observed_value) # 1 - stats.norm.cdf(z)",
      "",
      "# Arrondir la p-value à deux décimales",
      "p_value_rounded = round(p_value, 2)",
      "",
      "print(f'P-value: {p_value_rounded}')",
      "",
      "# ou pour les petits effectifs",
      "",
      "import numpy as np",
      "from scipy import stats",
      "",
      "# Pression artérielle avant et après le traitement",
      "avant = np.array([150, 160, 165, 145, 155, 152, 148, 160, 149, 158, 153, 150, 157, 155, 151, 159, 162, 156, 154, 149, 147, 150, 160, 155, 159, 158, 152, 150, 149, 160])",
      "apres = np.array([140, 155, 160, 142, 150, 148, 145, 152, 143, 154, 150, 147, 152, 150, 148, 153, 157, 151, 150, 145, 143, 148, 155, 150, 152, 155, 148, 146, 144, 155])",
      "",
      "# Test de Student pour échantillons appariés",
      "t_stat, p_value = stats.ttest_rel(avant, apres)",
      "# Test de Student pour échantillons indépendants",
      "t_stat, p_value = stats.ttest_ind(groupe_medicament, groupe_placebo)",
      "",
      "print(f'T-statistic: {t_stat}, P-value: {p_value}')",
      "",
      "# Niveau de signification",
      "alpha = 0.05",
      "",
      "# Conclusion",
      "if p_value <= alpha:",
      "    print(\"Nous rejetons l'hypothèse nulle. Le médicament a un effet significatif sur la pression artérielle.\")",
      "else:",
      "    print(\"Nous ne rejetons pas l'hypothèse nulle. Il n'y a pas de preuve suffisante que le médicament a un effet sur la pression artérielle.\")",
      ""
    ]
  },
  "ml_linear_regression_with_sklearn": {
    "prefix": "ml_linear_regression_with_sklearn",
    "body": [
      "# Define X and y",
      "X_train_scaled = data[['GrLivArea']]",
      "y_train = data['SalePrice']",
      "",
      "# ou",
      "# The Holdout Method  - Split Train/Test",
      "# Avec Sklearn",
      "",
      "## 0 - Analyse visuelle",
      "plt.scatter(X_train_scaled, y_train)",
      "",
      "## 1 - Instantiate the model",
      "model = LinearRegression()",
      "",
      "## 2 - Train the model on the data",
      "model.fit(X_train_scaled, y_train)",
      "",
      "### Afficher les coefficients",
      "print('Coefficients:', model.coef_) # View the model's slope (a)",
      "print('Intercept:', model.intercept_) # View the model's intercept (b)",
      "",
      "## 3 - Les scoring",
      "# R2 ⊂ [0,1]",
      "# The higher score the better the model",
      "print('R2:', model.score(X, y) ) # Evaluate the model's performance",
      "",
      "## 4 - Predict",
      "# Attention : si X est standardisé",
      "y_pred = model.predict(X_test_scaled)",
      ""
    ]
  },
  "ml_linear_regression_multivariate_with_statsmodels": {
    "prefix": "ml_linear_regression_multivariate_with_statsmodels",
    "body": [
      "import statsmodels.api as sm",
      "",
      "fig = plt.figure(figsize=(10,6))",
      "fig = sm.graphics.plot_partregress_grid(model2, fig=fig)",
      ""
    ]
  },
  "ml_logistic_regression_with_statsmodels": {
    "prefix": "ml_logistic_regression_with_statsmodels",
    "body": [
      "import pandas as pd",
      "import statsmodels.formula.api as smf",
      "",
      "# Charger le dataset Titanic",
      "titanic = sns.load_dataset('titanic').dropna()",
      "",
      "# Modèle de régression logistique simple",
      "model = smf.logit(formula='survived ~ fare', data=titanic).fit()",
      "",
      "# Résumé du modèle",
      "print(model.summary())",
      "",
      "# Coefficients du modèle",
      "print('Coefficients:', model.params)",
      "",
      "# Log-odds et odds ratio",
      "log_odds = model.params # Intercept : log(p/(1−p))",
      "odds_ratios = np.exp(log_odds)",
      "print('Odds Ratios:', odds_ratios)",
      "print('Probability:', 1 - odds_ratios) # Chance of ...",
      ""
    ],
    "description": "Régression Logistique Basique avec statsmodels"
  },
  "ml_logistic_regression_with_sklearn": {
    "prefix": "ml_logistic_regression_with_sklearn",
    "body": [
      "import pandas as pd",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.linear_model import LogisticRegression",
      "from sklearn.preprocessing import StandardScaler",
      "from sklearn.metrics import classification_report",
      "",
      "# Charger le dataset Titanic",
      "titanic = sns.load_dataset('titanic').dropna()",
      "",
      "# Define X and y",
      "# Sélection des features et de la target",
      "X = titanic[['fare']]",
      "y = titanic['survived']",
      "",
      "# Division des données en ensembles d'entraînement et de test",
      "# Before oversampling and scaling",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
      "",
      "## 0 - Preprocessing",
      "# Standardisation des données",
      "scaler = StandardScaler()",
      "X_train_scaled = scaler.fit_transform(X_train)",
      "X_test_scaled = scaler.transform(X_test)",
      "",
      "## 1 - Instantiate the model",
      "# Modèle de régression logistique",
      "log_reg = LogisticRegression(max_iter=1000)",
      "",
      "## 2 - Train the model on the data",
      "log_reg.fit(X_train_scaled, y_train)",
      "",
      "## 4 - Predict",
      "y_pred = log_reg.predict(X_test_scaled)",
      "",
      "# Rapport de classification",
      "print(classification_report(y_test, y_pred))",
      ""
    ],
    "description": "Régression Logistique Basique avec sklearn"
  },
  "multicollinearity_vif_with_statsmodels": {
    "prefix": "multicollinearity_vif_with_statsmodels",
    "body": [
      "import pandas as pd",
      "import seaborn as sns",
      "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif",
      "",
      "# Charger le dataset mpg",
      "mpg = sns.load_dataset('mpg').dropna().drop(columns=['origin', 'name', 'displacement'])",
      "",
      "# Calculer le VIF pour chaque feature",
      "mpg['lin_comb'] = 10 * mpg['cylinders'] - 0.3 * mpg['horsepower']",
      "mpg_scaled = mpg.apply(lambda x: (x - x.mean()) / x.std(), axis=0)",
      "",
      "df_vif = pd.DataFrame()",
      "df_vif['features'] = mpg_scaled.columns",
      "df_vif['vif_index'] = [vif(mpg_scaled.values, i) for i in range(mpg_scaled.shape[1])]",
      "",
      "print(df_vif)"
    ],
    "description": "Détection de la Multicolinéarité avec VIF avec statsmodels"
  },
  "multicollinearity_vif_with_sklearn": {
    "prefix": "multicollinearity_vif_with_sklearn",
    "body": [
      "import pandas as pd",
      "import seaborn as sns",
      "from sklearn.preprocessing import StandardScaler",
      "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif",
      "",
      "# Charger le dataset mpg",
      "mpg = sns.load_dataset('mpg').dropna().drop(columns=['origin', 'name', 'displacement'])",
      "",
      "# Ajouter une colonne lin_comb pour illustrer la multicolinéarité",
      "mpg['lin_comb'] = 10 * mpg['cylinders'] - 0.3 * mpg['horsepower']",
      "",
      "# Standardisation des données",
      "scaler = StandardScaler()",
      "mpg_scaled = pd.DataFrame(scaler.fit_transform(mpg), columns=mpg.columns)",
      "",
      "# Calculer le VIF pour chaque feature",
      "df_vif = pd.DataFrame()",
      "df_vif['features'] = mpg_scaled.columns",
      "df_vif['vif_index'] = [vif(mpg_scaled.values, i) for i in range(mpg_scaled.shape[1])]",
      "",
      "print(df_vif)"
    ],
    "description": "Détection de la Multicolinéarité avec VIF avec sklearn"
  },
  "library_machine_learning": {
    "prefix": "library_machine_learning",
    "body": [
      "######################################",
      "# Machine Learning                   #",
      "######################################",
      "",
      "# Holdout, Cross Val",
      "# ------------------------------------",
      "from sklearn.model_selection import train_test_split, cross_validate",
      "",
      "# Preprocessing",
      "# ------------------------------------",
      "",
      "## Imputers",
      "from sklearn.impute import SimpleImputer, KNNImputer",
      "",
      "## Scalers",
      "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler",
      "",
      "## Encoders",
      "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder",
      "",
      "## Pipeline and Column Transformers",
      "from sklearn.pipeline import Pipeline, make_pipeline",
      "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector",
      "from sklearn import set_config; set_config(display=\"diagram\")",
      "",
      "## Custom Classes",
      "from sklearn.base import TransformerMixin, BaseEstimator",
      "",
      "## Caching operations",
      "from tempfile import mkdtemp",
      "from shutil import rmtree",
      "",
      "# Feature selection",
      "# ------------------------------------",
      "from sklearn.feature_selection import mutual_info_regression, SelectPercentile",
      "",
      "# Evaluation",
      "# ------------------------------------",
      "",
      "## Scoring Methods",
      "from sklearn.metrics import make_scorer",
      "from sklearn.metrics import mean_squared_error, mean_squared_log_error",
      "from sklearn.model_selection import cross_validate, cross_val_score",
      "",
      "## Hyperparameter Tuning",
      "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV",
      "",
      "",
      "# Regression Models",
      "# ------------------------------------",
      "",
      "## Linear",
      "from sklearn.linear_model import Ridge, Lasso, LinearRegression, ElasticNet",
      "from sklearn.linear_model import SGDRegressor",
      "",
      "## Non-linear",
      "from sklearn.neighbors import KNeighborsRegressor",
      "from sklearn.tree import DecisionTreeRegressor",
      "",
      "## Support Vectors",
      "from sklearn.svm import SVR",
      "",
      "## Ensemble Methods",
      "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor",
      "from sklearn.ensemble import VotingRegressor, StackingRegressor",
      "",
      "## Masterpiece",
      "from xgboost import XGBRegressor",
      "",
      "## Catboost - TO DO",
      "",
      "## LightGBM - TO DO",
      ""
    ]
  },
  "ml_data_preprocess": {
    "prefix": "ml_data_preprocess",
    "body": [
      "import pandas as pd",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder",
      "",
      "# Charger le dataset",
      "data = pd.read_csv('data.csv')",
      "",
      "# Séparer les features et la target",
      "X = data.drop('target', axis=1)",
      "y = data['target']",
      "",
      "# Duplicates",
      "",
      "# Missing data (NaN)",
      "",
      "# Outliers",
      "",
      "# Discretizing",
      "",
      "# Diviser les données en ensembles d'entraînement et de test",
      "# Before oversampling and scaling",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
      "",
      "# Print the shapes of the resulting splits",
      "print(\"X_train shape:\", X_train.shape)",
      "print(\"X_test shape:\", X_test.shape)",
      "print(\"y_train shape:\", y_train.shape)",
      "print(\"y_test shape:\", y_test.shape)",
      "",
      "# Feature scaling (Numerical)",
      "# Standardiser les données",
      "scaler = StandardScaler()",
      "X_train_scaled = scaler.fit_transform(X_train)",
      "X_test_scaled = scaler.transform(X_test)",
      "",
      "# Encoding (Categorical)",
      "# Encodage One-Hot des variables catégorielles",
      "encoder = OneHotEncoder()",
      "X_train_encoded = encoder.fit_transform(X_train_scaled)",
      "X_test_encoded = encoder.transform(X_test_scaled)",
      "",
      "# Dataset balancing",
      "",
      "# Feature creation",
      "",
      "# Feature selection, Modelling and Feature Permutation",
      ""
    ],
    "description": "Préparation des données : séparation, standardisation et encodage"
  },
  "ml_logistic_regression_evaluate": {
    "prefix": "ml_logistic_regression_evaluate",
    "body": [
      "from sklearn.linear_model import LogisticRegression",
      "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc",
      "import matplotlib.pyplot as plt",
      "",
      "## 4 - Les scoring",
      "",
      "# Rapport de classification",
      "print(classification_report(y_test, y_pred))",
      "",
      "# Matrice de confusion",
      "conf_matrix = confusion_matrix(y_test, y_pred)",
      "print('Confusion Matrix:\\n', conf_matrix)",
      "",
      "# Calcul du ROC AUC",
      "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])",
      "print('ROC AUC:', roc_auc)",
      "",
      "# Courbe ROC (false positive rate, true positive rate)",
      "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])",
      "",
      "# Affichage de la courbe ROC",
      "plt.figure()",
      "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)",
      "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')",
      "plt.xlim([0.0, 1.0])",
      "plt.ylim([0.0, 1.05])",
      "plt.xlabel('False Positive Rate')",
      "plt.ylabel('True Positive Rate')",
      "plt.title('Receiver Operating Characteristic')",
      "plt.legend(loc='lower right')",
      "plt.show()",
      ""
    ],
    "description": "Évaluation de la régression logistique avec Scikit-Learn"
  },
  "ml_linear_regression_evaluate": {
    "prefix": "ml_linear_regression_evaluate",
    "body": [
      "import numpy as np",
      "from sklearn.linear_model import LinearRegression",
      "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error",
      "",
      "# Use MSE when you need to penalize large errors more strictly than small ones.",
      "mse = mean_squared_error(y, y_pred)",
      "",
      "# Use RMSE when you need to penalize large errors, but see it in the unit of the target.",
      "rmse = math.sqrt(mse)",
      "",
      "# Use MAE when all errors, large or small, have equal importance and you need easily interpretable results",
      "mae = mean_absolute_error(y, y_pred)",
      "",
      "# Use Max Error when you want to limit the magnitude of the errors.",
      "max_error = max_error(y, y_pred)",
      "",
      "# Use R2 as a general \"goodness-of-fit\" metric to compare performance across changes to models and the data",
      "rsquared = r2_score(y, y_pred)",
      "",
      "print('MSE =', round(mse, 2))",
      "print('RMSE =', round(rmse, 2))",
      "print('MAE =', round(mae, 2))",
      "print('R2 =', round(rsquared, 2))",
      "print('Max Error =', round(max_error, 2))",
      ""
    ],
    "description": "Évaluation de la régression linéaire avec Scikit-Learn"
  },
  "preprocess_nan_simple_imputer": {
    "prefix": "preprocess_nan_simple_imputer",
    "body": [
      "import pandas as pd",
      "from sklearn.impute import SimpleImputer",
      "",
      "# Exemple de DataFrame avec des valeurs manquantes",
      "data = pd.DataFrame({'Pesos': [1.2, 2.3, None, 4.5, 5.7]})",
      "",
      "# Afficher le DataFrame original",
      "print('Original DataFrame:')",
      "print(data)",
      "",
      "# Be careful : Missing data does not necessarily mean a lack of information!",
      "data.isna().sum().sort_values(ascending=False)",
      "data.isnull().sum().sort_values(ascending=False)/len(data)",
      "",
      "# Instancier un objet SimpleImputer avec la stratégie de votre choix",
      "imputer = SimpleImputer(strategy='mean')",
      "",
      "# Appeler la méthode 'fit' sur l'objet",
      "imputer.fit(data[['Pesos']])",
      "",
      "# Appeler la méthode 'transform' sur l'objet",
      "data['Pesos'] = imputer.transform(data[['Pesos']])",
      "",
      "# Afficher le DataFrame après imputation",
      "print('DataFrame après imputation:')",
      "print(data)",
      "",
      "# La moyenne est stockée dans la mémoire du transformateur",
      "print('Moyenne utilisée pour imputation:', imputer.statistics_)",
      ""
    ],
    "description": "Utilisation de SimpleImputer pour imputer les valeurs manquantes avec la moyenne"
  },
  "preprocess_nan_knn_imputer_example": {
    "prefix": "preprocess_nan_knn_imputer",
    "body": [
      "import pandas as pd",
      "from sklearn.impute import KNNImputer",
      "",
      "# Exemple de DataFrame avec des valeurs manquantes",
      "data = pd.DataFrame({",
      "    'A': [1, 2, None, 4, 5],",
      "    'B': [5, None, 3, 4, None],",
      "    'C': [2, 3, 4, None, 1]",
      "})",
      "",
      "# Afficher le DataFrame original",
      "print('Original DataFrame:')",
      "print(data)",
      "",
      "# Be careful : Missing data does not necessarily mean a lack of information!",
      "data.isna().sum().sort_values(ascending=False)",
      "data.isnull().sum().sort_values(ascending=False)/len(data)",
      "",
      "# Créer l'objet KNNImputer",
      "# Utilise les k plus proches voisins pour imputer les valeurs manquantes",
      "imputer = KNNImputer(n_neighbors=2)",
      "",
      "# Appliquer l'imputation sur le DataFrame",
      "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)",
      "",
      "# Afficher le DataFrame après imputation",
      "print('DataFrame après imputation:')",
      "print(data_imputed)",
      ""
    ],
    "description": "Utilisation de KNNImputer pour imputer les valeurs manquantes en utilisant les k plus proches voisins"
  },
  "preprocess_nan_preserve": {
    "prefix": "preprocess_nan_preserve",
    "body": [
      "# Be careful : Missing data does not necessarily mean a lack of information!",
      "data.isna().sum().sort_values(ascending=False)",
      "data.isnull().sum().sort_values(ascending=False)/len(data)",
      "",
      "## Preserve the NaNs and replace them with meaningful values",
      "######## Less than 30% of missing values",
      "data['Alley'].replace(np.nan, \"NoAlley\", inplace=True)",
      "data['Pesos'].replace(np.nan, data['Pesos'].mean())",
      ""
    ]
  },
  "preprocess_nan_drop": {
    "prefix": "preprocess_nan_drop",
    "body": [
      "# Be careful : Missing data does not necessarily mean a lack of information!",
      "data.isna().sum().sort_values(ascending=False)",
      "data.isnull().sum().sort_values(ascending=False)/len(data)",
      "",
      "## Drop the column entirely",
      "######## More than 30% of missing values",
      "data.drop(columns=['WallMat','Test'], inplace=True)",
      "data.dropna(subset=['WallMat','Test'])",
      ""
    ]
  },
  "preprocess_duplicate": {
    "prefix": "preprocess_duplicate",
    "body": [
      "import pandas as pd",
      "",
      "# Check the data",
      "data[[\"GrLivArea\",\"SalePrice\"]].head(10)",
      "",
      "# Check number of rows before removing duplicates",
      "len(data)",
      "",
      "# Check whether a row is a duplicated version of a previous row",
      "data.duplicated()",
      "",
      "# Compute the number of duplicated rows",
      "duplicate_rows = data.duplicated().sum()",
      "print(f\"Number of duplicate rows: {duplicate_rows}\")",
      "",
      "# Remove duplicates",
      "data = data.drop_duplicates()",
      "cars = cars.drop_duplicates().reset_index(drop = True) # if no need to remember the previous index",
      "",
      "# Check new number of rows",
      "len(data)",
      ""
    ]
  },
  "preprocess_outliers_drop": {
    "prefix": "preprocess_outliers_drop",
    "body": [
      "# Detecting Outliers",
      "data[['GrLivArea']].boxplot();",
      "data['GrLivArea'].min()",
      "",
      "# Hadling Outliers",
      "### Outliers can be an opinion.",
      "### Dropping Outliers",
      "boolean_mask = (data['GrLivArea']>0) & (data['GrLivArea']<5000)",
      "data = data[boolean_mask].reset_index(drop=True)",
      "data[['GrLivArea']].boxplot();",
      ""
    ]
  },
  "preprocess_balancing_downsampling_manual": {
    "prefix": "preprocess_balancing_downsampling_manual",
    "body": [
      "# Separate into class 0 and class 1",
      "df_class_0 = df[df['default'] == 0.0]",
      "df_class_1 = df[df['default'] == 1.0]",
      "",
      "# Downsample class 0 to match the number of samples in class 1",
      "df_class_0_downsampled = df_class_0.sample(n=len(df_class_1), random_state=42)",
      "",
      "# Combine downsampled class 0 with class 1",
      "df_balanced = pd.concat([df_class_0_downsampled, df_class_1])",
      "",
      "# Shuffle the DataFrame (optional but recommended)",
      "df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)",
      "",
      "print(df['default'].value_counts())",
      ""
    ]
  },
  "preprocess_encoder_ordinal": {
    "prefix": "preprocess_encoder_ordinal",
    "body": [
      "import pandas as pd",
      "from sklearn.preprocessing import OrdinalEncoder",
      "",
      "# Exemple de DataFrame",
      "data = {",
      "    'classes': ['good', 'average', 'bad', 'good', 'bad']",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Instancier l'Ordinal Encoder",
      "### Be careful : create a false relationship",
      "ordinal_encoder = OrdinalEncoder(categories=[['bad', 'average', 'good']])",
      "",
      "# Ajuster l'encodeur sur les données",
      "ordinal_encoder.fit(df[['classes']])",
      "",
      "# Afficher les catégories apprises",
      "print('Catégories apprises:', ordinal_encoder.categories_)",
      "",
      "# Transformer les catégories en nombres ordonnés",
      "df['encoded_classes'] = ordinal_encoder.transform(df[['classes']])",
      "",
      "# Afficher le DataFrame transformé",
      "print(df)",
      ""
    ],
    "description": "Utilisation de OrdinalEncoder pour encoder des variables catégorielles ordinales"
  },
  "preprocess_encoder_onehot": {
    "prefix": "preprocess_encoder_onehot",
    "body": [
      "import pandas as pd",
      "from sklearn.preprocessing import OneHotEncoder",
      "",
      "# Exemple de DataFrame",
      "data = {",
      "    'Alley': ['Gravel', 'Paved', 'Gravel', 'Paved', 'Paved']",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Instancier le OneHotEncoder",
      "ohe = OneHotEncoder(sparse_output=False, drop=\"if_binary\")",
      "",
      "# Ajuster l'encodeur sur les données",
      "ohe.fit(df[['Alley']])",
      "",
      "# Transformer la colonne 'Alley'",
      "df[ohe.get_feature_names_out()] = ohe.transform(df[['Alley']])",
      "",
      "# Supprimer la colonne 'Alley' qui a été encodée",
      "df.drop(columns=['Alley'], inplace=True)",
      "",
      "# Afficher le DataFrame transformé",
      "print(df)",
      ""
    ],
    "description": "Utilisation de OneHotEncoder pour encoder des variables catégorielles nominales"
  },
  "preprocess_encoder_onehot_with_keras": {
    "prefix": "preprocess_encoder_onehot_with_keras",
    "body": [
      "from tensorflow.keras.utils import to_categorical",
      "",
      "y_cat_train = to_categorical(y_train, num_classes=10)",
      "y_cat_test = to_categorical(y_test, num_classes=10)",
      "",
      "y_cat_train.shape",
      ""
    ],
    "description": "Utilisation de OneHotEncoder pour encoder des variables catégorielles nominales"
  },
  "preprocess_discretizing_cut_with_pandas": {
    "prefix": "preprocess_discretizing_cut_with_pandas",
    "body": [
      "import pandas as pd",
      "",
      "# Exemple de DataFrame",
      "data = {'continuous_feature': [1.0, 2.5, 3.3, 4.7, 6.1, 8.9, 9.5]}",
      "df = pd.DataFrame(data)",
      "",
      "# Définir des intervalles spécifiques pour les bins",
      "bins = [0, 3, 6, 10]",
      "labels = ['low', 'medium', 'high']",
      "",
      "# Utiliser pandas.cut avec des intervalles définis",
      "df['binned_feature'] = pd.cut(df['continuous_feature'], bins=bins, labels=labels)",
      "",
      "# Afficher le DataFrame transformé",
      "print(df)",
      ""
    ],
    "description": "Utilisation de pandas.cut pour discrétiser des données continues en intervalles"
  },
  "preprocess_discretizing_quantile_transformer": {
    "prefix": "preprocess_discretizing_quantile_transformer",
    "body": [
      "import pandas as pd",
      "from sklearn.preprocessing import QuantileTransformer",
      "",
      "# Exemple de DataFrame",
      "data = {'continuous_feature': [1.0, 2.5, 3.3, 4.7, 6.1, 8.9, 9.5]}",
      "df = pd.DataFrame(data)",
      "",
      "# Instancier le QuantileTransformer",
      "quantile_transformer = QuantileTransformer(n_quantiles=4, output_distribution='uniform')",
      "",
      "# Ajuster le transformateur sur les données",
      "quantile_transformer.fit(df[['continuous_feature']])",
      "",
      "# Transformer les données continues en quantiles",
      "df['quantile_feature'] = quantile_transformer.transform(df[['continuous_feature']])",
      "",
      "# Afficher le DataFrame transformé",
      "print(df)"
    ],
    "description": "Utilisation de QuantileTransformer pour transformer des données continues en données discrètes selon des quantiles"
  },
  "preprocess_discretizing_binarizer": {
    "prefix": "preprocess_discretizing_binarizer",
    "body": [
      "import pandas as pd",
      "from sklearn.preprocessing import Binarizer",
      "",
      "# Exemple de DataFrame",
      "data = {'continuous_feature': [1.0, 2.5, 3.3, 4.7, 6.1, 8.9, 9.5]}",
      "df = pd.DataFrame(data)",
      "",
      "# Instancier le Binarizer avec un seuil de 5",
      "binarizer = Binarizer(threshold=5)",
      "",
      "# Transformer les données continues en données binaires",
      "df['binary_feature'] = binarizer.transform(df[['continuous_feature']])",
      "",
      "# Afficher le DataFrame transformé",
      "print(df)"
    ],
    "description": "Utilisation de Binarizer pour transformer des données continues en données binaires"
  },
  "preprocess_discretizing_kbins_discretizer": {
    "prefix": "preprocess_discretizing_kbins_discretizer",
    "body": [
      "import pandas as pd",
      "from sklearn.preprocessing import KBinsDiscretizer",
      "",
      "# Exemple de DataFrame",
      "data = {'continuous_feature': [1.0, 2.5, 3.3, 4.7, 6.1, 8.9, 9.5]}",
      "df = pd.DataFrame(data)",
      "",
      "# Instancier le KBinsDiscretizer",
      "kbins = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')",
      "",
      "# Ajuster l'encodeur sur les données",
      "kbins.fit(df[['continuous_feature']])",
      "",
      "# Transformer les données continues en catégories",
      "df['binned_feature'] = kbins.transform(df[['continuous_feature']])",
      "",
      "# Afficher le DataFrame transformé",
      "print(df)"
    ],
    "description": "Utilisation de KBinsDiscretizer pour transformer des données continues en données discrètes"
  },
  "preprocess_feature_correlation_matrix": {
    "prefix": "preprocess_feature_correlation_matrix",
    "body": [
      "# Heatmap of pairwise correlations",
      "correlation_matrix = data.select_dtypes('number').corr()",
      "",
      "# Convert the correlation matrix into a DataFrame",
      "corr_df = correlation_matrix.stack().reset_index()",
      "",
      "# Rename the columns",
      "corr_df.columns = ['feature_1','feature_2', 'correlation']",
      "",
      "# Remove \"self correlations\"",
      "no_self_correlation = (corr_df['feature_1'] != corr_df['feature_2'])",
      "corr_df = corr_df[no_self_correlation]",
      "",
      "# Compute the absolute correlation",
      "corr_df['absolute_correlation'] = np.abs(corr_df['correlation'])",
      "",
      "# Showe the top 5 most correlated pairs of feature",
      "corr_df.sort_values(by=\"absolute_correlation\", ascending=False).head(5*2)",
      ""
    ]
  },
  "preprocess_feature_permutation_importance": {
    "prefix": "preprocess_feature_permutation_importance",
    "body": [
      "import pandas as pd",
      "from sklearn.ensemble import RandomForestClassifier",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.inspection import permutation_importance",
      "",
      "# Exemple de DataFrame",
      "data = {",
      "    'feature1': [1, 2, 3, 4, 5],",
      "    'feature2': [10, 20, 30, 40, 50],",
      "    'target': [0, 1, 0, 1, 0]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Séparer les features et la target",
      "X = df.drop(columns=['target'])",
      "y = df['target']",
      "",
      "# Diviser les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
      "",
      "# Créer et ajuster le modèle",
      "model = RandomForestClassifier(random_state=42)",
      "model.fit(X_train, y_train)",
      "",
      "# Calculer l'importance des caractéristiques par permutation",
      "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)",
      "",
      "# Unstack results showing the decrease in performance after shuffling features",
      "importance_df = pd.DataFrame(np.vstack((X.columns, \\",
      "                                        permutation_score.importances_mean)).T)",
      "importance_df.columns=['feature','score decrease']",
      "",
      "# Show the important features",
      "importance_df.sort_values(by=\"score decrease\", ascending = False)",
      ""
    ],
    "description": "Utilisation de permutation_importance pour évaluer l'importance des caractéristiques"
  },
  "ml_regressor_kneighbors": {
    "prefix": "ml_regressor_kneighbors",
    "body": [
      "import pandas as pd",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.neighbors import KNeighborsRegressor",
      "from sklearn.metrics import mean_squared_error, r2_score",
      "",
      "# Exemple de DataFrame",
      "data = {",
      "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8],",
      "    'feature2': [5, 6, 7, 8, 9, 10, 11, 12],",
      "    'target': [1.2, 2.4, 3.1, 4.8, 5.0, 6.2, 7.3, 8.8]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Séparer les features et la target",
      "X = df.drop(columns=['target'])",
      "y = df['target']",
      "",
      "# Diviser les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
      "",
      "# Instancier le KNeighborsRegressor",
      "knn = KNeighborsRegressor(n_neighbors=3) # k : lower -> overfitting",
      "",
      "# Ajuster le modèle",
      "knn.fit(X_train, y_train)",
      "",
      "# Prédire les valeurs de test",
      "y_pred = knn.predict(X_test)",
      "",
      "# Évaluer le modèle",
      "mse = mean_squared_error(y_test, y_pred)",
      "r2 = r2_score(y_test, y_pred)",
      "print('Mean Squared Error:', round(mse, 2))",
      "print('R² Score:', round(r2, 2))"
    ],
    "description": "Utilisation de KNeighborsRegressor pour la régression"
  },
  "ml_classifier_kneighbors": {
    "prefix": "ml_classifier_kneighbors",
    "body": [
      "import pandas as pd",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.neighbors import KNeighborsClassifier",
      "from sklearn.metrics import accuracy_score, classification_report",
      "",
      "# Exemple de DataFrame",
      "data = {",
      "    'feature1': [5, 2, 4, 6, 2, 4, 7, 8],",
      "    'feature2': [3, 4, 2, 3, 5, 6, 2, 1],",
      "    'target': [0, 1, 0, 0, 1, 1, 0, 1]",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Séparer les features et la target",
      "X = df.drop(columns=['target'])",
      "y = df['target']",
      "",
      "# Diviser les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
      "",
      "# Instancier le KNeighborsClassifier",
      "knn = KNeighborsClassifier(n_neighbors=3) # k : lower -> overfitting",
      "",
      "# Ajuster le modèle",
      "knn.fit(X_train, y_train)",
      "",
      "# Prédire les valeurs de test",
      "y_pred = knn.predict(X_test)",
      "",
      "# Évaluer le modèle",
      "accuracy = accuracy_score(y_test, y_pred)",
      "report = classification_report(y_test, y_pred)",
      "print('Accuracy:', round(accuracy, 2))",
      "print('Classification Report:\\n', report)"
    ],
    "description": "Utilisation de KNeighborsClassifier pour la classification"
  },
  "ml_dummy_regressor_baseline": {
    "prefix": "ml_dummy_regressor_baseline",
    "body": [
      "from sklearn.model_selection import train_test_split",
      "from sklearn.dummy import DummyRegressor",
      "import pandas as pd",
      "",
      "# Exemple de DataFrame",
      "data = {'age': [19, 18, 28, 33, 32],",
      "        'bmi': [27.9, 33.77, 33.0, 22.705, 28.88],",
      "        'children': [0, 1, 3, 0, 0],",
      "        'smoker': [True, False, False, False, False],",
      "        'charges': [16884.924, 1725.5523, 4449.462, 21984.47061, 3866.8552]}",
      "data = pd.DataFrame(data)",
      "",
      "# Préparer X et y",
      "X = data[['age', 'bmi', 'children', 'smoker']]",
      "y = data['charges']",
      "",
      "# Diviser les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Modèle baseline",
      "baseline_model = DummyRegressor(strategy='mean')",
      "baseline_model.fit(X_train, y_train)",
      "",
      "# Score du modèle",
      "print('Baseline Score:', baseline_model.score(X_test, y_test))"
    ],
    "description": "Utilisation de DummyRegressor pour établir un score de référence pour la régression"
  },
  "ml_classification_evaluate_accuracy": {
    "prefix": "ml_classification_evaluate",
    "body": [
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix",
      "",
      "# Exemple de valeurs prédites et réelles",
      "y_true = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1]",
      "y_pred = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]",
      "",
      "# Calcul des métriques",
      "accuracy = accuracy_score(y_true, y_pred)",
      "precision = precision_score(y_true, y_pred) # Reducing false positives : safety",
      "recall = recall_score(y_true, y_pred) # Reducing false negatives : frauds",
      "f1 = f1_score(y_true, y_pred)",
      "conf_matrix = confusion_matrix(y_true, y_pred)",
      "",
      "# Afficher les résultats",
      "print('Accuracy =', round(accuracy, 2))",
      "print('Precision =', round(precision, 2))",
      "print('Recall =', round(recall, 2))",
      "print('F1 score =', round(f1, 2))",
      "print('Confusion Matrix =\\n', conf_matrix)"
    ],
    "description": "Calcul et affichage des métriques de classification"
  },
  "ml_classification_precision_recall_curve": {
    "prefix": "ml_classification_precision_recall_curve",
    "body": [
      "from sklearn.linear_model import LogisticRegression",
      "from sklearn.model_selection import cross_val_predict",
      "from sklearn.metrics import precision_recall_curve",
      "import matplotlib.pyplot as plt",
      "import pandas as pd",
      "",
      "# Exemple de DataFrame",
      "data = {'price_range': ['cheap', 'expensive', 'cheap', 'cheap', 'expensive'],",
      "        'feature1': [10, 20, 30, 40, 50],",
      "        'feature2': [15, 25, 35, 45, 55]}",
      "data = pd.DataFrame(data)",
      "",
      "# Encoder la cible",
      "from sklearn.preprocessing import LabelEncoder",
      "le = LabelEncoder()",
      "data['price_range_encoded'] = le.fit_transform(data['price_range'])",
      "",
      "# Modèle et prédiction",
      "X = data[['feature1', 'feature2']]",
      "y = data['price_range_encoded']",
      "model = LogisticRegression()",
      "data['proba_cheap'], data['proba_expensive'] = cross_val_predict(",
      "    model,",
      "    X,",
      "    y,",
      "    cv=5,",
      "    method='predict_proba').T",
      "",
      "# Calcul des courbes précision-rappel",
      "precision, recall, threshold = precision_recall_curve(y, data['proba_expensive'])",
      "",
      "print(f'precision- {precision[:5]}')",
      "print(f'recall- {recall[:5]}')",
      "print(f'threshold- {threshold[:5]}')",
      "",
      "scores = pd.DataFrame({'threshold':threshold,",
      "                        'precision': precision[:-1],",
      "                        'recall':recall[:-1]}) # Store in a dataframe",
      "scores",
      "",
      "# Tracer la courbe précision-rappel",
      "plt.plot(recall, precision)",
      "plt.ylabel('Precision')",
      "plt.xlabel('Recall')",
      "plt.title('Precision-Recall Curve')",
      "plt.show()",
      "",
      "# Find the threshold that guarantees a 0.8 recall score",
      "scores[scores['recall'] >= 0.8].threshold.max()",
      ""
    ],
    "description": "Tracer la courbe précision-rappel"
  },
  "ml_regressor_ridge": {
    "prefix": "ml_regressor_ridge",
    "body": [
      "from sklearn.linear_model import Ridge",
      "",
      "# Données d'exemple",
      "X = [[1, 2], [2, 3], [3, 4], [4, 5]]",
      "y = [2, 3, 4, 5]",
      "",
      "# Instancier le modèle",
      "# Réduction de variance et gestion de la multicolinéarité",
      "ridge = Ridge(alpha=0.2)",
      "",
      "# Ajuster le modèle",
      "ridge.fit(X, y)",
      "",
      "# Prédire",
      "y_pred = ridge.predict(X)",
      "",
      "# Afficher les coefficients",
      "coefs = pd.DataFrame({",
      "    \"coef_ridge\": pd.Series(ridge.coef_, index = X.columns),",
      "})",
      "",
      "coefs\\",
      "    .applymap(lambda x: int(x))\\",
      "    .style.applymap(lambda x: 'color: red' if x == 0 else 'color: black')",
      ""
    ],
    "description": "Utilisation de Ridge Regression"
  },
  "ml_regressor_lasso": {
    "prefix": "ml_regressor_lasso",
    "body": [
      "from sklearn.linear_model import Lasso",
      "",
      "# Données d'exemple",
      "X = [[1, 2], [2, 3], [3, 4], [4, 5]]",
      "y = [2, 3, 4, 5]",
      "",
      "# Instancier le modèle",
      "# Réduction de variance et sélection de variables",
      "lasso = Lasso(alpha=0.2)",
      "",
      "# Ajuster le modèle",
      "lasso.fit(X, y)",
      "",
      "# Prédire",
      "y_pred = lasso.predict(X)",
      "",
      "# Afficher les coefficients",
      "coefs = pd.DataFrame({",
      "    \"coef_lasso\": pd.Series(lasso.coef_, index = X.columns),",
      "})",
      "",
      "coefs\\",
      "    .applymap(lambda x: int(x))\\",
      "    .style.applymap(lambda x: 'color: red' if x == 0 else 'color: black')",
      ""
    ],
    "description": "Utilisation de Lasso Regression"
  },
  "ml_tuning_randomizedsearch_cv": {
    "prefix": "ml_tuning_randomizedsearch_cv",
    "body": [
      "from sklearn.model_selection import RandomizedSearchCV",
      "from sklearn.svm import SVC",
      "from scipy.stats import uniform",
      "",
      "# Données d'exemple",
      "X = [[1, 2], [2, 3], [3, 4], [4, 5]]",
      "y = [0, 1, 0, 1]",
      "",
      "# Définir le modèle et les paramètres",
      "model = SVC()",
      "param_dist = {",
      "    'C': uniform(0.1, 10),",
      "    'kernel': ['linear', 'rbf']",
      "}",
      "",
      "# Configurer RandomizedSearchCV",
      "random_search = RandomizedSearchCV(",
      "    model,",
      "    param_dist,",
      "    n_iter=100,",
      "    cv=5,",
      "    random_state=42",
      ")",
      "",
      "# Ajuster le modèle",
      "random_search.fit(X, y)",
      "",
      "# Afficher les meilleurs paramètres",
      "print('Meilleurs paramètres:', random_search.best_params_)",
      "print('Meilleur score:', random_search.best_score_)",
      ""
    ],
    "description": "Utilisation de RandomizedSearchCV pour l'optimisation du SVM"
  },
  "ml_svm_kernel_tricks": {
    "prefix": "ml_svm_kernel_tricks",
    "body": [
      "from sklearn import datasets",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.svm import SVC",
      "from sklearn.metrics import accuracy_score",
      "",
      "# Chargement des données d'exemple",
      "iris = datasets.load_iris()",
      "X = iris.data",
      "y = iris.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Instancier le modèle SVM avec différents noyaux",
      "kernels = ['linear', 'poly', 'rbf', 'sigmoid']",
      "for kernel in kernels:",
      "    model = SVC(kernel=kernel, C=1.0, gamma='scale')",
      "    model.fit(X_train, y_train)",
      "    y_pred = model.predict(X_test)",
      "    accuracy = accuracy_score(y_test, y_pred)",
      "    print(f'Kernel: {kernel}, Accuracy: {accuracy}')"
    ],
    "description": "Utilisation de différents Kernel Tricks avec SVM"
  },
  "ml_classifier_sgd": {
    "prefix": "ml_classifier_sgd",
    "body": [
      "from sklearn import datasets",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.linear_model import SGDClassifier",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score",
      "",
      "# Chargement des données d'exemple",
      "iris = datasets.load_iris()",
      "X = iris.data",
      "y = iris.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Instancier le modèle SGDClassifier",
      "### Sample > 100K",
      "model = SGDClassifier(",
      "    loss='hinge', # Ce qui équivaut à un SVM linéaire",
      "    penalty='l2',",
      "    alpha=0.0001, # Une valeur plus grande augmente la régularisation",
      "    max_iter=1000, # Nombre maximal d'itérations",
      "    tol=1e-3 # Tolérance pour le critère d'arrêt",
      ")",
      "",
      "# Ajuster le modèle",
      "model.fit(X_train, y_train)",
      "",
      "# Prédire les valeurs de test",
      "y_pred = model.predict(X_test)",
      "",
      "# Évaluer le modèle",
      "accuracy = accuracy_score(y_test, y_pred)",
      "precision = precision_score(y_test, y_pred, average='macro')",
      "recall = recall_score(y_test, y_pred, average='macro')",
      "f1 = f1_score(y_test, y_pred, average='macro')",
      "",
      "print(f'Accuracy: {accuracy}')",
      "print(f'Precision: {precision}')",
      "print(f'Recall: {recall}')",
      "print(f'F1 Score: {f1}')",
      ""
    ],
    "description": "Utilisation de SGDClassifier pour la classification avec descente de gradient stochastique"
  },
  "pipeline_preprocess": {
    "prefix": "pipeline_preprocess",
    "body": [
      "from sklearn.pipeline import Pipeline",
      "from sklearn.preprocessing import FunctionTransformer",
      "from sklearn.impute import SimpleImputer",
      "from sklearn.preprocessing import StandardScaler",
      "# Visualizing Pipelines in HTML",
      "from sklearn import set_config; set_config(display='diagram')",
      "",
      "# Create a transformer that compresses data to 2 digits",
      "rounder = FunctionTransformer(lambda array: np.round(array, decimals=2))",
      "",
      "pipeline = Pipeline([",
      "    ('imputer', SimpleImputer(strategy='median')),",
      "    ('scaler', StandardScaler()),",
      "    ('rounder', rounder))",
      "])",
      "",
      "pipeline.fit(X_train[['age']])",
      "X_train_transformed = pipeline.transform(X_train[['age']])",
      "print(X_train_transformed)",
      "",
      "# Get your features' names",
      "pipeline.get_feature_names_out()",
      "pd.DataFrame(",
      "    X_train_transformed,",
      "    columns=pipeline.get_feature_names_out()",
      ").head()",
      ""
    ],
    "description": "Pipeline de prétraitement avec imputation et scaling"
  },
  "pipeline_column_transformer": {
    "prefix": "pipeline_column_transformer",
    "body": [
      "from sklearn.compose import ColumnTransformer",
      "from sklearn.compose import make_column_transformer",
      "from sklearn.pipeline import Pipeline",
      "from sklearn.impute import SimpleImputer",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder",
      "# Visualizing Pipelines in HTML",
      "from sklearn import set_config; set_config(display='diagram')",
      "",
      "num_transformer = Pipeline([",
      "    ('imputer', SimpleImputer(strategy='mean')),",
      "    ('scaler', StandardScaler())",
      "])",
      "",
      "cat_transformer = OneHotEncoder(handle_unknown='ignore')",
      "",
      "preprocessor = ColumnTransformer(",
      "    transformers=[",
      "        ('num', num_transformer, ['age', 'bmi']),",
      "        ('cat', cat_transformer, ['smoker', 'region'])",
      "    ],",
      "    remainder='passthrough', # Les autres colonnes sont conservées",
      "    n_jobs=-1",
      ")",
      "# preprocessor = make_column_transformer(",
      "#     (num_transformer, ['age', 'bmi']),",
      "#     (cat_transformer, ['smoker', 'region']),",
      "#     remainder='drop', # Les autres colonnes sont supprimées",
      "#     n_jobs=-1",
      "# )",
      "",
      "X_train_transformed = preprocessor.fit_transform(X_train)",
      "print(X_train_transformed)",
      ""
    ],
    "description": "Column Transformer pour imputer et scaler les valeurs numériques, encoder les valeurs catégorielles"
  },
  "pipeline_model_full": {
    "prefix": "pipeline_model_full",
    "body": [
      "from sklearn.pipeline import Pipeline",
      "from sklearn.compose import ColumnTransformer",
      "from sklearn.impute import SimpleImputer",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder",
      "from sklearn.linear_model import Ridge",
      "# Visualizing Pipelines in HTML",
      "from sklearn import set_config; set_config(display='diagram')",
      "",
      "num_transformer = Pipeline([",
      "    ('imputer', SimpleImputer()),",
      "    ('scaler', StandardScaler())",
      "])",
      "",
      "cat_transformer = OneHotEncoder(handle_unknown='ignore')",
      "",
      "preprocessor = ColumnTransformer([",
      "    (num_transformer, ['age', 'bmi']),",
      "    (cat_transformer, ['smoker', 'region'])",
      "], remainder='passthrough')",
      "",
      "pipeline = Pipeline([",
      "    ('preprocessor', preprocessor),",
      "    ('model', Ridge())",
      "])",
      "",
      "pipeline.fit(X_train, y_train)",
      "y_pred = pipeline.predict(X_test)",
      "print(y_pred)",
      "",
      "# Access the components of a Pipeline with `named_steps`",
      "pipeline.named_steps.keys()",
      "",
      "# Check intermediate steps",
      "print(\"Before preprocessing, X_train.shape = \")",
      "print(X_train.shape)",
      "print(\"After preprocessing, X_train_preprocessed.shape = \")",
      "pipeline.named_steps[\"columntransformer\"].fit_transform(X_train).shape",
      ""
    ],
    "description": "Intégration de modèles dans les pipelines"
  },
  "pipeline_cross_validate": {
    "prefix": "pipeline_cross_validate",
    "body": [
      "from sklearn.model_selection import cross_val_score",
      "from sklearn.pipeline import Pipeline",
      "from sklearn.compose import ColumnTransformer",
      "from sklearn.impute import SimpleImputer",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder",
      "from sklearn.linear_model import Ridge",
      "# Visualizing Pipelines in HTML",
      "from sklearn import set_config; set_config(display='diagram')",
      "",
      "num_transformer = Pipeline([",
      "    ('imputer', SimpleImputer()),",
      "    ('scaler', StandardScaler())",
      "])",
      "",
      "cat_transformer = OneHotEncoder(handle_unknown='ignore')",
      "",
      "preprocessor = ColumnTransformer([",
      "    (num_transformer, ['age', 'bmi']),",
      "    (cat_transformer, ['smoker', 'region'])",
      "], remainder='passthrough')",
      "",
      "pipeline = Pipeline([",
      "    ('preprocessor', preprocessor),",
      "    ('model', Ridge())",
      "])",
      "",
      "scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')",
      "print(scores.mean())",
      ""
    ],
    "description": "Validation croisée de la pipeline"
  },
  "pipeline_grid_search": {
    "prefix": "pipeline_grid_search",
    "body": [
      "from sklearn.model_selection import GridSearchCV",
      "from sklearn.pipeline import Pipeline",
      "from sklearn.compose import ColumnTransformer",
      "from sklearn.impute import SimpleImputer",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder",
      "from sklearn.linear_model import Ridge",
      "# Visualizing Pipelines in HTML",
      "from sklearn import set_config; set_config(display='diagram')",
      "",
      "num_transformer = Pipeline([",
      "    ('imputer', SimpleImputer()),",
      "    ('scaler', StandardScaler())",
      "])",
      "",
      "cat_transformer = OneHotEncoder()",
      "",
      "preprocessor = ColumnTransformer([",
      "    (num_transformer, ['age', 'bmi']),",
      "    (cat_transformer, ['smoker', 'region'])",
      "], remainder='passthrough')",
      "",
      "pipeline = Pipeline([",
      "    ('preprocessor', preprocessor),",
      "    ('model', Ridge())",
      "])",
      "",
      "# Which parameters of the pipeline are GridSearch-able?",
      "pipeline.get_params()",
      "",
      "param_grid = {",
      "    # Access any component of the Pipeline",
      "    # and any available hyperparamater you want to optimize",
      "    'preprocessor__num__imputer__strategy': ['mean', 'median'],",
      "    'model__alpha': [0.1, 0.5, 1, 5, 10]",
      "}",
      "",
      "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')",
      "grid_search.fit(X_train, y_train)",
      "print(grid_search.best_params_)",
      ""
    ],
    "description": "Recherche des hyperparamètres optimaux avec Grid Search"
  },
  "pipeline_custom_transformer": {
    "prefix": "pipeline_custom_transformer",
    "body": [
      "#########################################",
      "# 1 - Code the CustomStandardizer Class #",
      "#########################################",
      "",
      "# TransformerMixin inheritance is used to create fit_transform() method from fit() and transform()",
      "from sklearn.base import TransformerMixin, BaseEstimator",
      "",
      "# Bonus: allows us to raise a NotFittedError when one calls the transform method before fitting the instance",
      "from sklearn.exceptions import NotFittedError",
      "",
      "class CustomStandardizer(TransformerMixin, BaseEstimator):",
      "",
      "    def __init__(self):",
      "        pass",
      "",
      "    def fit(self, X, y=None):",
      "        '''",
      "        Stores what needs to be stored as instance attributes.",
      "        ReturnS \"self\" to allow chaining fit and transform.",
      "        '''",
      "        self.means = X.mean()",
      "        self.stds = X.std(ddof=0)",
      "",
      "        # Return self to allow chaining & fit_transform",
      "        return self",
      "",
      "    def transform(self, X, y=None):",
      "        if not (hasattr(self, \"means\") and hasattr(self, \"stds\")):",
      "            raise NotFittedError(\"This CustomStandardScaler instance is not fitted yet. Call 'fit' with the appropriate arguments before using this estimator.\")",
      "",
      "        # Standardization",
      "        standardized_features = (X - self.means) / self.stds",
      "",
      "        return standardized_features",
      "",
      "    def inverse_transform(self, X, y=None):",
      "        if not (hasattr(self, \"means\") and hasattr(self, \"stds\")):",
      "            raise NotFittedError(\"This CustomStandardScaler instance is not fitted yet. Call 'fit' with the appropriate arguments before using this estimator.\")",
      "",
      "        return X * self.stds + self.means",
      "",
      "#########################################",
      "# 2 - Fit the CustomStandardizer Class  #",
      "#########################################",
      "",
      "custom_standardizer = CustomStandardizer()",
      "custom_standardizer.fit(X_train)",
      "",
      "#########################################",
      "# 3 - Transform                         #",
      "#########################################",
      "",
      "X_train_transformed = custom_standardizer.transform(X_train)",
      "X_test_transformed = custom_standardizer.transform(X_test)",
      "",
      "print(\"X_train_transformed:\")",
      "display(X_train_transformed)",
      "",
      "print(\"X_test_transformed:\")",
      "X_test_transformed",
      ""
    ]
  },
  "pipeline_custom_transformer_cyclical": {
    "prefix": "pipeline_custom_transformer_cyclical",
    "body": [
      "# TransformerMixin inheritance is used to create fit_transform() method from fit() and transform()",
      "from sklearn.base import TransformerMixin, BaseEstimator",
      "",
      "class CyclicalFeatures(TransformerMixin, BaseEstimator):",
      "",
      "    def __init__(self):",
      "        pass",
      "",
      "    def fit(self, X, y=None):",
      "        return self",
      "",
      "    def transform(self, X, y=None):",
      "        sin =  np.sin(2 * np.pi * (X.MoSold - 1) / 12)",
      "        cos = np.cos(2 * np.pi * (X.MoSold - 1) / 12)",
      "        self.transformed = pd.DataFrame({'sin_MoSold':sin,'cos_MoSold':cos})",
      "        return self.transformed.reset_index(drop=True)",
      "",
      "    def get_feature_names_out(self):",
      "        return self.columns",
      "",
      "cyclical_features_transformed = CyclicalFeatures().fit_transform(X_train)",
      ""
    ]
  },
  "pipeline_feature_union": {
    "prefix": "pipeline_feature_union",
    "body": [
      "import pandas as pd",
      "from sklearn.compose import ColumnTransformer",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer",
      "from sklearn.pipeline import Pipeline, FeatureUnion",
      "from sklearn.impute import SimpleImputer",
      "",
      "# Exemple de DataFrame",
      "data = {",
      "    'age': [25, 30, 35, 40],",
      "    'bmi': [22.0, 24.5, 28.0, 30.5],",
      "    'categorical_feature': ['A', 'B', 'A', 'C']",
      "}",
      "df = pd.DataFrame(data)",
      "",
      "# Pipeline pour les caractéristiques numériques",
      "numeric_pipeline = Pipeline(steps=[",
      "    ('imputer', SimpleImputer(strategy='mean')),",
      "    ('scaler', StandardScaler())",
      "])",
      "",
      "# Pipeline pour les caractéristiques catégorielles",
      "categorical_pipeline = Pipeline(steps=[",
      "    ('imputer', SimpleImputer(strategy='most_frequent')),",
      "    ('onehot', OneHotEncoder())",
      "])",
      "",
      "# Transformer personnalisé pour créer une nouvelle caractéristique",
      "bmi_age_ratio_constructor = FunctionTransformer(lambda df: pd.DataFrame(df['bmi'] / df['age']), validate=False)",
      "",
      "# Utilisation de ColumnTransformer pour combiner les transformations",
      "preprocessor = ColumnTransformer(",
      "    transformers=[",
      "        ('num', numeric_pipeline, ['age', 'bmi']),",
      "        ('cat', categorical_pipeline, ['categorical_feature'])",
      "    ]",
      ")",
      "",
      "# Utilisation de FeatureUnion pour combiner les transformations avec la nouvelle caractéristique",
      "union = FeatureUnion([",
      "    ('preprocess', preprocessor),",
      "    ('bmi_age_ratio', bmi_age_ratio_constructor)",
      "])",
      "",
      "# Ajuster et transformer les données",
      "transformed_data = union.fit_transform(df)",
      "print(transformed_data)",
      ""
    ],
    "description": "Utilisation de FeatureUnion pour combiner plusieurs transformations de caractéristiques et ajouter une nouvelle caractéristique dérivée"
  },
  "ml_confusion_matrix_display": {
    "prefix": "ml_confusion_matrix_display",
    "body": [
      "import numpy as np",
      "import matplotlib.pyplot as plt",
      "from sklearn.datasets import load_iris",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.linear_model import LogisticRegression",
      "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay",
      "",
      "# Chargement des données",
      "iris = load_iris()",
      "X = iris.data",
      "y = iris.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Instancier et ajuster le modèle",
      "model = LogisticRegression(max_iter=200)",
      "model.fit(X_train, y_train)",
      "",
      "# Prédire les valeurs de test",
      "y_pred = model.predict(X_test)",
      "",
      "# Calculer la matrice de confusion",
      "cm = confusion_matrix(y_test, y_pred)",
      "",
      "# Visualiser la matrice de confusion",
      "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)",
      "disp.plot(cmap=plt.cm.Blues)",
      "plt.title('Matrice de Confusion')",
      "plt.show()"
    ],
    "description": "Visualisation de la matrice de confusion avec ConfusionMatrixDisplay"
  },
  "save_model_with_pickle": {
    "prefix": "save_model_with_pickle",
    "body": [
      "import pickle",
      "from sklearn.linear_model import LogisticRegression",
      "from sklearn.datasets import load_iris",
      "from sklearn.model_selection import train_test_split",
      "",
      "# Chargement des données",
      "iris = load_iris()",
      "X = iris.data",
      "y = iris.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Instancier et ajuster le modèle",
      "model = LogisticRegression(max_iter=200)",
      "model.fit(X_train, y_train)",
      "",
      "# Sauvegarder le modèle avec pickle",
      "pickle.dump(model, open('model.pkl', 'wb'))",
      "",
      "print('Modèle sauvegardé sous le nom model.pkl')",
      ""
    ],
    "description": "Sauvegarder un modèle entraîné avec pickle"
  },
  "load_model_with_pickle": {
    "prefix": "load_model_with_pickle",
    "body": [
      "import pickle",
      "",
      "# Charger le modèle depuis le fichier pickle",
      "loaded_model = pickle.load(open('model.pkl', 'rb'))",
      "",
      "print('Modèle chargé depuis model.pkl')",
      "",
      "# Exemple de prédiction avec le modèle chargé",
      "y_pred = loaded_model.predict(X_test) # loaded_model.predict_proba(df)[0][1]",
      "print('Prédictions:', y_pred)"
    ],
    "description": "Charger un modèle sauvegardé avec pickle"
  },
  "save_model_with_keras": {
    "prefix": "save_model_with_keras",
    "body": [
      "from tensorflow.keras import models",
      "",
      "# Sauvegarder le modèle avec keras",
      "models.save_model(model, 'my_model')",
      "",
      "print('Modèle sauvegardé sous le nom my_model')",
      ""
    ],
    "description": "Sauvegarder un modèle entraîné avec keras"
  },
  "load_model_with_keras": {
    "prefix": "load_model_with_keras",
    "body": [
      "from tensorflow.keras import models",
      "",
      "# Charger le modèle",
      "loaded_model = models.load_model('my_model')",
      "",
      "print('Modèle chargé depuis my_model')",
      "",
      "# Exemple de prédiction avec le modèle chargé",
      "y_pred = loaded_model.predict(X_test) # loaded_model.predict_proba(df)[0][1]",
      "print('Prédictions:', y_pred)",
      ""
    ],
    "description": "Charger un modèle sauvegardé avec keras"
  },
  "os_path_notebook": {
    "prefix": "os_path_notebook",
    "body": [
      "import os",
      "",
      "# Obtenir le répertoire de travail actuel",
      "current_dir = os.getcwd()",
      "print(f\"Répertoire de travail actuel : {current_dir}\")",
      "",
      "# Construire un chemin de fichier relatif",
      "path = os.path.join(current_dir, 'data-tuning-pipeline', 'pipeline.pkl')",
      "print(f\"Chemin du fichier : {path}\")",
      "",
      "# Vérifier l'existence du fichier",
      "if os.path.exists(path):",
      "    print(\"Le fichier existe.\")",
      "else:",
      "    print(\"Le fichier n'existe pas.\")"
    ],
    "description": "Utilisation de os et path dans un notebook pour gérer les chemins de fichiers"
  },
  "os_path_module": {
    "prefix": "os_path_module",
    "body": [
      "import os",
      "",
      "# Obtenir le répertoire du fichier de module actuel",
      "current_dir = os.path.dirname(os.path.abspath(__file__))",
      "print(f\"Répertoire du fichier de module actuel : {current_dir}\")",
      "",
      "# Construire un chemin de fichier relatif",
      "path = os.path.join(current_dir, 'data-tuning-pipeline', 'pipeline.pkl')",
      "print(f\"Chemin du fichier : {path}\")",
      "",
      "# Vérifier l'existence du fichier",
      "if os.path.exists(path):",
      "    print(\"Le fichier existe.\")",
      "else:",
      "    print(\"Le fichier n'existe pas.\")"
    ],
    "description": "Utilisation de os et path dans un fichier de module pour gérer les chemins de fichiers"
  },
  "os_create_directory": {
    "prefix": "os_create_directory",
    "body": [
      "import os",
      "",
      "# Chemin du répertoire à créer",
      "dir_path = os.path.join(current_dir, 'new_directory')",
      "",
      "# Vérifier l'existence du répertoire et le créer s'il n'existe pas",
      "if not os.path.exists(dir_path):",
      "    os.makedirs(dir_path)",
      "    print(f\"Répertoire créé : {dir_path}\")",
      "else:",
      "    print(f\"Le répertoire existe déjà : {dir_path}\")"
    ],
    "description": "Créer un répertoire s'il n'existe pas"
  },
  "os_list_files": {
    "prefix": "os_list_files",
    "body": [
      "import os",
      "",
      "# Chemin du répertoire à lister",
      "dir_path = os.path.join(current_dir, 'data-tuning-pipeline')",
      "",
      "# Lister tous les fichiers dans le répertoire",
      "if os.path.exists(dir_path):",
      "    files = os.listdir(dir_path)",
      "    print(f\"Fichiers dans le répertoire {dir_path} : {files}\")",
      "else:",
      "    print(f\"Le répertoire n'existe pas : {dir_path}\")"
    ],
    "description": "Lister les fichiers dans un répertoire"
  },
  "os_list_files_with_walk": {
    "prefix": "os_list_files_with_walk",
    "body": [
      "import os",
      "",
      "# Chemin du répertoire à lister",
      "dir_path = os.path.join(current_dir, 'data-tuning-pipeline')",
      "",
      "# Lister tous les fichiers dans le répertoire et ses sous-répertoires",
      "if os.path.exists(dir_path):",
      "    for root, dirs, files in os.walk(dir_path):",
      "        print(f'Parcours du répertoire : {root}')",
      "        print(f'Sous-répertoires : {dirs}')",
      "        print(f'Fichiers : {files}')",
      "else:",
      "    print(f'Le répertoire n'existe pas : {dir_path}')"
    ],
    "description": "Lister les fichiers dans un répertoire et ses sous-répertoires avec os.walk"
  },
  "ml_regressor_tpot": {
    "prefix": "ml_regressor_tpot",
    "body": [
      "import numpy as np",
      "import pandas as pd",
      "import os",
      "from tpot import TPOTRegressor",
      "from sklearn.datasets import load_boston",
      "from sklearn.model_selection import train_test_split",
      "",
      "# Charger les données",
      "boston = load_boston()",
      "X = boston.data",
      "y = boston.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Instancier le TPOTRegressor",
      "tpot = TPOTRegressor(",
      "    generations=5,",
      "    population_size=50,",
      "    verbosity=2,",
      "    random_state=42,",
      "    scoring='r2',",
      "    n_jobs=-1,",
      "    cv=2",
      ")",
      "",
      "# Entraîner TPOT sur les données d'entraînement",
      "tpot.fit(X_train, y_train)",
      "",
      "# Évaluer le modèle sur les données de test",
      "score = tpot.score(X_test, y_test)",
      "print(f'Score sur les données de test: {score}')",
      "",
      "# Exporter le meilleur pipeline",
      "tpot.export(os.path.join(os.getcwd(),'best_pipeline.py')",
      ""
    ],
    "description": "Utilisation de TPOTRegressor pour optimiser un pipeline de régression"
  },
  "ml_classifier_decision_tree": {
    "prefix": "ml_classifier_decision_tree",
    "body": [
      "from sklearn.datasets import load_iris",
      "import pandas as pd",
      "import numpy as np",
      "",
      "# Charger les données Iris",
      "iris = load_iris()",
      "data = pd.DataFrame(data=np.c_[iris['data'], iris['target']],",
      "                    columns=iris['feature_names'] + ['target'])",
      "data.drop(columns=['sepal length (cm)', 'sepal width (cm)'], inplace=True)",
      "",
      "X = data.drop(columns=['target']).values",
      "y = data.target.values",
      "",
      "# Importer et entraîner DecisionTreeClassifier",
      "from sklearn.tree import DecisionTreeClassifier",
      "tree_clf = DecisionTreeClassifier(",
      "    max_depth=2,",
      "    random_state=2",
      ")",
      "tree_clf.fit(X, y)",
      "",
      "# Visualiser l'arbre de décision",
      "import graphviz",
      "from sklearn.tree import export_graphviz",
      "",
      "export_graphviz(",
      "    tree_clf,",
      "    out_file=\"iris_tree.dot\",",
      "    feature_names=data.drop(columns=['target']).columns,",
      "    class_names=['0', '1', '2'],",
      "    rounded=True,",
      "    filled=True",
      ")",
      "",
      "with open(\"iris_tree.dot\") as f:",
      "    dot_graph = f.read()",
      "    display(graphviz.Source(dot_graph))",
      "",
      "# Prédire la classe d'un nouveau point",
      "print(tree_clf.predict([[4, 1]]))  # Prédiction",
      "print(tree_clf.predict_proba([[4, 1]]))  # Probabilités",
      "",
      "# Calculer l'index de Gini pour le nœud racine",
      "### Calculates the impurity -> The lower the better",
      "gini_root = 1 - (50/150)**2 - (50/150)**2 - (50/150)**2",
      "",
      "# Calculer l'index de Gini pour une feuille",
      "gini_leaf = 1 - 0**2 - (49/54)**2 - (5/54)**2",
      ""
    ]
  },
  "ml_classifier_decision_tree_plot": {
    "prefix": "ml_classifier_decision_tree_plot",
    "body": [
      "from sklearn.tree import DecisionTreeClassifier",
      "from ipywidgets import interact",
      "",
      "#@interact(max_depth=10)",
      "def plot_classifier(max_depth):",
      "    clf = DecisionTreeClassifier(max_depth=max_depth)",
      "    clf.fit(X_moon, y_moon)",
      "    plot_decision_regions(X_moon, y_moon, classifier=clf)",
      "",
      "plot_classifier(max_depth=10)",
      ""
    ]
  },
  "ml_classifier_random_forest": {
    "prefix": "ml_classifier_random_forest",
    "body": [
      "from sklearn.ensemble import RandomForestClassifier",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.datasets import load_iris",
      "",
      "# Charger les données",
      "iris = load_iris()",
      "X = iris.data",
      "y = iris.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Instancier et entraîner RandomForestClassifier",
      "forest_clf = RandomForestClassifier(",
      "    n_estimators=100,",
      "    random_state=42",
      ")",
      "forest_clf.fit(X_train, y_train)",
      "",
      "# Évaluer le modèle",
      "score = forest_clf.score(X_test, y_test)",
      "print(f'Score sur les données de test: {score}')",
      ""
    ]
  },
  "ml_classifier_bagging": {
    "prefix": "ml_classifier_bagging",
    "body": [
      "from sklearn.ensemble import BaggingClassifier, BaggingRegressor",
      "from sklearn.neighbors import KNeighborsClassifier",
      "",
      "# Reduce variance/overfitting but lack of interpretability",
      "weak_learner = KNeighborsClassifier(n_neighbors=3)",
      "bagged_model = BaggingClassifier(weak_learner, n_estimators=40)",
      "",
      "bagged_model.fit(X_moon, y_moon)",
      "plot_decision_regions(X_moon, y_moon, classifier=bagged_model)",
      ""
    ]
  },
  "ml_classifier_boosting": {
    "prefix": "ml_classifier_boosting",
    "body": [
      "from sklearn.ensemble import AdaBoostClassifier",
      "",
      "# Reduces Biais/underfitting but focus on outliers",
      "adaboost = AdaBoostClassifier(",
      "    DecisionTreeClassifier(max_depth=3),",
      "    n_estimators=50",
      ")",
      "",
      "adaboost.fit(X_moon, y_moon)",
      "",
      "plot_decision_regions(X_moon, y_moon, classifier=model)",
      ""
    ]
  },
  "ml_classifier_gradient_boosting": {
    "prefix": "ml_classifier_gradient_boosting",
    "body": [
      "from sklearn.ensemble import GradientBoostingClassifier",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.datasets import load_iris",
      "",
      "# Charger les données",
      "iris = load_iris()",
      "X = iris.data",
      "y = iris.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Instancier et entraîner GradientBoostingClassifier",
      "gboost_clf = GradientBoostingClassifier(",
      "    n_estimators=100,",
      "    learning_rate=1.0,",
      "    max_depth=1,",
      "    random_state=42",
      ")",
      "gboost_clf.fit(X_train, y_train)",
      "",
      "# Évaluer le modèle",
      "score = gboost_clf.score(X_test, y_test)",
      "print(f'Score sur les données de test: {score}')",
      ""
    ]
  },
  "ml_regressor_xgboost": {
    "prefix": "ml_regressor_xgboost",
    "body": [
      "from xgboost import XGBRegressor",
      "",
      "xgb_reg = XGBRegressor(",
      "    max_depth=10,",
      "    n_estimators=100,",
      "    learning_rate=0.1",
      ")",
      "",
      "xgb_reg.fit(X_train, y_train,",
      "    # evaluate loss at each iteration",
      "    eval_set=[(X_train, y_train), (X_val, y_val)],",
      "    # stop iterating when eval loss increases 5 times in a row",
      "    early_stopping_rounds=5",
      ")",
      "",
      "y_pred = xgb_reg.predict(X_val)",
      ""
    ]
  },
  "ml_classifier_stacking": {
    "prefix": "ml_classifier_stacking",
    "body": [
      "from sklearn.ensemble import StackingClassifier",
      "from sklearn.ensemble import VotingClassifier",
      "from sklearn.linear_model import LogisticRegression",
      "from sklearn.svm import SVC",
      "from sklearn.model_selection import train_test_split",
      "from sklearn.datasets import load_iris",
      "",
      "# Charger les données",
      "iris = load_iris()",
      "X = iris.data",
      "y = iris.target",
      "",
      "# Séparer les données en ensembles d'entraînement et de test",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "",
      "# Définir les modèles de base",
      "estimators = [",
      "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),",
      "    ('svr', SVC(kernel='linear', probability=True)),",
      "    ('svc', SVC(kernel='linear', probability=True, random_state=42))",
      "]",
      "",
      "# Instancier et entraîner StackingClassifier",
      "stack_clf = StackingClassifier(",
      "    estimators=estimators,",
      "    final_estimator=LogisticRegression()",
      ")",
      "# voting_clf = VotingClassifier(",
      "#     estimators=estimators,",
      "#     voting = 'soft', # to use predict_proba of each classifier before voting",
      "#     weights = [1,1,1] # to equally weight forest and logreg in the vote",
      "# )",
      "",
      "stack_clf.fit(X_train, y_train)",
      "",
      "# Évaluer le modèle",
      "score = stack_clf.score(X_test, y_test)",
      "print(f'Score sur les données de test: {score}')",
      ""
    ]
  },
  "ml_unsupervised_pca": {
    "prefix": "ml_unsupervised_pca",
    "body": [
      "import numpy as np",
      "import pandas as pd",
      "from sklearn.decomposition import PCA",
      "from sklearn.datasets import load_iris",
      "",
      "# Charger les données Iris",
      "iris = load_iris()",
      "X = iris.data",
      "",
      "# Fit a PCA with only 3 components",
      "pca3 = PCA(n_components=3).fit(X)",
      "",
      "# Project your data into 3 dimensions",
      "X_proj3 = pd.DataFrame(pca3.fit_transform(X), columns=['PC1', 'PC2', 'PC3'])",
      "",
      "# We have \"compressed\" our dataset in 3D",
      "X_proj3",
      "",
      "# Afficher les résultats",
      "print(X_proj3.head())",
      "print(f'Variance expliquée par chaque composante : {pca3.explained_variance_ratio_}')",
      "",
      "# Decompress",
      "X_reconstructed = pca3.inverse_transform(X_proj3)",
      "X_reconstructed.shape",
      ""
    ],
    "description": "Réduction de dimensions avec PCA"
  },
  "ml_unsupervised_kmeans": {
    "prefix": "ml_unsupervised_kmeans",
    "body": [
      "import numpy as np",
      "import pandas as pd",
      "from sklearn.cluster import KMeans",
      "from sklearn.datasets import load_iris",
      "",
      "# Charger les données Iris",
      "iris = load_iris()",
      "X = iris.data",
      "",
      "# Instancier et entraîner KMeans",
      "kmeans = KMeans(",
      "    n_clusters=3,",
      "    random_state=42",
      ")",
      "kmeans.fit(X)",
      "",
      "# Ajouter les labels de clusters aux données",
      "df = pd.DataFrame(X, columns=iris.feature_names)",
      "df['cluster'] = kmeans.labels_",
      "",
      "# The 3 centroids' coordinates (expressed in the space of PCs)",
      "print(kmeans.cluster_centers_.shape)",
      "print(df.head())",
      "print(f'Centroids des clusters : {kmeans.cluster_centers_}')",
      "",
      "# Plot the cluster",
      "plt.figure(figsize=(13,5))",
      "",
      "plt.subplot(1,2,1)",
      "plt.scatter(X.iloc[:,0], X.iloc[:,1], c=y)",
      "plt.title('True labels')",
      "plt.xlabel('PC 1')",
      "plt.ylabel('PC 2');",
      "",
      "plt.subplot(1,2,2)",
      "plt.scatter(X.iloc[:,0], X.iloc[:,1], c=kmeans.labels_)",
      "plt.title('KMeans clustering')",
      "plt.xlabel('PC 1')",
      "plt.ylabel('PC 2');",
      ""
    ],
    "description": "Clustering avec K-Means"
  },
  "ml_unsupervised_kmeans_choose_k_inertia": {
    "prefix": "ml_unsupervised_kmeans_choose_k_inertia",
    "body": [
      "import numpy as np",
      "import pandas as pd",
      "from sklearn.cluster import KMeans",
      "from sklearn.datasets import load_iris",
      "import matplotlib.pyplot as plt",
      "",
      "# Charger les données Iris",
      "iris = load_iris()",
      "X = iris.data",
      "",
      "# Calculer les inerties pour différents k",
      "inertias = []",
      "ks = range(1, 11)",
      "",
      "for k in ks:",
      "    km_test = KMeans(n_clusters=k, random_state=42).fit(X)",
      "    inertias.append(km_test.inertia_)",
      "",
      "# Visualiser les inerties",
      "plt.figure(figsize=(8, 5))",
      "plt.plot(ks, inertias, '-o')",
      "plt.xlabel('Nombre de clusters k')",
      "plt.ylabel('Inertie')",
      "plt.title('Méthode du coude pour déterminer le k optimal')",
      "plt.xticks(ks)",
      "plt.show()"
    ],
    "description": "Choisir l'hyperparamètre k de l'inertie en utilisant la méthode du coude"
  },
  "dl_manual_layer": {
    "prefix": "dl_manual_layer",
    "body": [
      "import numpy as np",
      "",
      "def activation(x):",
      "    if x > 0:",
      "        return x",
      "    else:",
      "        return 0",
      "",
      "def linreg_1(X):",
      "    return -3 + 2.1*X[0] - 1.2*X[1] + 0.3*X[2] + 1.3*X[3]",
      "",
      "def linreg_2(X):",
      "    return -5 - 0.1*X[0] + 1.2*X[1] + 4.9*X[2] - 3.1*X[3]",
      "",
      "def linreg_3(X):",
      "    return -8 + 0.4*X[0] + 2.6*X[1] - 2.5*X[2] + 3.8*X[3]",
      "",
      "def linreg_next_layer(X):",
      "    return 5.1 + 1.1*X[0] - 4.1*X[1] - 0.7*X[2]",
      "",
      "def activation_next_layer(x):",
      "    # sigmoid activation for clasification task!",
      "    return 1. / (1 + np.exp(-x))",
      "",
      "def neural_net_predictor(X):",
      "    out_1 = activation(linreg_1(X))",
      "    out_2 = activation(linreg_2(X))",
      "    out_3 = activation(linreg_3(X))",
      "    outs = [out_1, out_2, out_3]",
      "    y_pred = activation_next_layer(linreg_next_layer(outs))",
      "    return y_pred",
      "",
      "X = [1., -3.1, -7.2, 2.1]",
      "y_pred = neural_net_predictor(X)",
      "print(f'Probability of being a dog: {y_pred}')",
      ""
    ],
    "description": "Réseau de neurones"
  },
  "dl_dense_with_keras": {
    "prefix": "dl_dense_with_keras",
    "body": [
      "from tensorflow.keras import Sequential, layers",
      "",
      "# STEP 1: ARCHITECTURE",
      "model = Sequential()",
      "",
      "# Input layer",
      "model.add(layers.Dense(10, activation='relu', input_dim=4))  # First layer needs the size of your input",
      "",
      "# Hidden dense layer",
      "model.add(layers.Dense(10, activation='relu'))",
      "model.add(layers.Dropout(rate=0.2))  # The rate is the percentage of neurons that are \"killed\"",
      "model.add(layers.Dense(10, activation='relu'))",
      "",
      "# Predictive dense layer",
      "model.add(layers.Dense(1, activation='linear')) ### REGRESSION WITH 1 OUTPUT",
      "# model.add(layers.Dense(16, activation='linear')) ### REGRESSION WITH 16 OUTPUTS",
      "# model.add(layers.Dense(1, activation='sigmoid')) ### CLASSIFICATION WITH 2 CLASSES",
      "# model.add(layers.Dense(14, activation='softmax') ### CLASSIFICATION WITH 14 CLASSES",
      "",
      "# Counting number of parameters",
      "# print(\"manual calculation = \", (10 * 4 + 10) + (10 + 1)) # Par couche : pois + biais = ni * ni+1 + ni+1",
      "model.summary()",
      ""
    ],
    "description": "Définition de l'architecture du modèle avec Keras"
  },
  "dl_dense_regularized_with_keras": {
    "prefix": "dl_dense_regularized_with_keras",
    "body": [
      "from tensorflow.keras import regularizers, Sequential, layers",
      "",
      "reg_l1 = regularizers.L1(0.01)",
      "reg_l2 = regularizers.L2(0.01)",
      "reg_l1_l2 = regularizers.l1_l2(l1=0.005, l2=0.0005)",
      "",
      "# STEP 1: ARCHITECTURE",
      "model = Sequential()",
      "",
      "# Input layer",
      "model.add(layers.Dense(10, activation='relu', input_dim=4))  # First layer needs the size of your input",
      "",
      "# Hidden dense layer",
      "model.add(layers.Dense(50, activation='relu', kernel_regularizer=reg_l1))",
      "model.add(layers.Dense(20, activation='relu', bias_regularizer=reg_l2))",
      "model.add(layers.Dense(10, activation='relu', activity_regularizer=reg_l1_l2))",
      "",
      "# Predictive dense layer",
      "model.add(layers.Dense(1, activation='linear')) ### REGRESSION WITH 1 OUTPUT",
      "# model.add(layers.Dense(16, activation='linear')) ### REGRESSION WITH 16 OUTPUTS",
      "# model.add(layers.Dense(1, activation='sigmoid')) ### CLASSIFICATION WITH 2 CLASSES",
      "# model.add(layers.Dense(14, activation='softmax') ### CLASSIFICATION WITH 14 CLASSES",
      "",
      "# Counting number of parameters",
      "# print(\"manual calculation = \", (10 * 4 + 10) + (10 + 1)) # Par couche : pois + biais = ni * ni+1 + ni+1",
      "model.summary()",
      ""
    ],
    "description": "Définition de l'architecture du modèle avec Keras"
  },
  "dl_training_with_keras": {
    "prefix": "dl_training_with_keras",
    "body": [
      "from tensorflow.keras.callbacks import EarlyStopping",
      "",
      "# STEP 2: OPTIMIZATION METHODS",
      "",
      "# REGRESSION",
      "model.compile(loss='mse',",
      "              optimizer='adam',",
      "              metrics=['mae']) # MSE, RMSE, RMSLE, R-squared",
      "",
      "# # CLASSIFICATION WITH 2 CLASSES",
      "# model.compile(loss='binary_crossentropy',",
      "#               optimizer='adam',",
      "#               metrics=['accuracy']) # Precision, Recall, F1-score, ROC-AUC",
      "",
      "# # CLASSIFICATION WITH N (let's say 14) CLASSES",
      "# model.compile(loss='categorical_crossentropy',",
      "#               optimizer='adam',",
      "#               metrics=['accuracy', 'precision'])",
      "",
      "# SETP 3: DATA AND FITTING METHODS",
      "# X_train and y_train should be prepared beforehand",
      "",
      "es = EarlyStopping(",
      "    patience=20,",
      "    restore_best_weights=True",
      ")",
      "",
      "history = model.fit(X_train,",
      "          y_train,",
      "          validation_split=0.3, # LAST 30% of train indexes are used as validation",
      "          #validation_data=(x_test, y_test),",
      "          batch_size=32, # 16 or 32 is most for real-word data",
      "          epochs=10, # The larger the batch size, the more epochs you will need",
      "          shuffle=True,",
      "          callbacks=[es],",
      "          verbose=1",
      ")",
      "",
      "# Evaluate returns [loss, metrics]",
      "model.evaluate(X_test_preprocessed, y_test)",
      "",
      "# Predicted probabilities",
      "model.predict(X_test_preprocessed)",
      ""
    ],
    "description": "Compilation du modèle avec Keras"
  },
  "dl_training_finetuned_with_keras": {
    "prefix": "dl_training_finetuned_with_keras",
    "body": [
      "from tensorflow.keras.callbacks import EarlyStopping",
      "from tensorflow.keras import optimizers",
      "",
      "# STEP 2: OPTIMIZATION METHODS",
      "",
      "# use Keras optimizer objects for fine-tuning",
      "opt = optimizers.Adam(",
      "    learning_rate=0.01,",
      "    beta_1=0.9,",
      "    beta_2=0.99",
      ")",
      "",
      "# use Keras metric objects for fine-tuning",
      "metric = keras.metrics.AUC(",
      "    num_thresholds = 200,",
      "    curve='ROC', # or curve='PR'",
      ")",
      "",
      "# use Keras metric objects for fine-tuning",
      "loss = keras.losses.BinaryCrossentropy(...)",
      "",
      "model.compile(loss=loss,",
      "              optimizer=opt,",
      "              metrics=metric) # MSE, RMSE, RMSLE, R-squared",
      "",
      "# SETP 3: DATA AND FITTING METHODS",
      "# X_train and y_train should be prepared beforehand",
      "",
      "es = EarlyStopping(",
      "    patience=20,",
      "    restore_best_weights=True",
      ")",
      "",
      "history = model.fit(X_train,",
      "          y_train,",
      "          validation_split=0.3, # LAST 30% of train indexes are used as validation",
      "          batch_size=32, # 16 or 32 is most for real-word data",
      "          epochs=10, # The larger the batch size, the more epochs you will need",
      "          shuffle=True,",
      "          callbacks=[es]",
      ")",
      "",
      "# Evaluate returns [loss, metrics]",
      "model.evaluate(X_test_preprocessed, y_test)",
      "",
      "# Predicted probabilities",
      "model.predict(X_test_preprocessed)",
      ""
    ],
    "description": "Compilation du modèle avec Keras"
  },
  "dl_training_custom": {
    "prefix": "dl_training_custom",
    "body": [
      "from tensorflow.keras.callbacks import EarlyStopping",
      "",
      "# STEP 2: OPTIMIZATION METHODS",
      "",
      "# Custom metrics",
      "def custom_mse(y_true, y_pred):",
      "    squared_diff = tf.square(y_true - y_pred)",
      "    return tf.reduce_mean(squared_diff)",
      "",
      "# Custom losses",
      "def custom_mse(y_true, y_pred):",
      "    squared_diff = tf.square(y_true - y_pred)",
      "    return tf.reduce_mean(squared_diff)",
      "",
      "model.compile(loss=custom_mse,",
      "              optimizer='adam',",
      "              metrics=[custom_mse]) # MSE, RMSE, RMSLE, R-squared",
      "",
      "# SETP 3: DATA AND FITTING METHODS",
      "# X_train and y_train should be prepared beforehand",
      "",
      "es = EarlyStopping(",
      "    patience=20,",
      "    restore_best_weights=True",
      ")",
      "",
      "history = smodel.fit(X_train,",
      "          y_train,",
      "          validation_split=0.3, # LAST 30% of train indexes are used as validation",
      "          batch_size=32, # 16 or 32 is most for real-word data",
      "          epochs=10, # The larger the batch size, the more epochs you will need",
      "          shuffle=True,",
      "          callbacks=[es]",
      ")",
      "",
      "# Evaluate returns [loss, metrics]",
      "model.evaluate(X_test_preprocessed, y_test)",
      "",
      "# Predicted probabilities",
      "model.predict(X_test_preprocessed)",
      ""
    ],
    "description": "Compilation du modèle avec Keras"
  },
  "dl_cnn_with_keras": {
    "prefix": "dl_cnn_with_keras",
    "body": [
      "from tensorflow.keras import Sequential, layers",
      "",
      "model = Sequential()",
      "",
      "# strides : the kernel moving by a fixed number of pixels",
      "# padding='valid' : no-padding, the output is smaller than the input",
      "# padding='same' : padded with enough empty pixels to get an output of the same size as the input",
      "model.add(layers.Conv2D(32, kernel_size=(3, 3), strides=(1,1), padding='valid', activation='relu', input_shape=(64, 64, 3)))",
      "",
      "model.add(layers.MaxPooling2D(pool_size=(2, 2))) # Reduce the size of the output with the maximum intensity value",
      "model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))",
      "model.add(layers.MaxPooling2D(pool_size=(2, 2)))",
      "model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))",
      "model.add(layers.AveragePooling2D(pool_size=(2, 2)))",
      "model.add(layers.Flatten())",
      "",
      "model.add(layers.Dense(64, activation='relu'))",
      "model.add(layers.Dense(10, activation='softmax'))  # Output layer for 10 classes",
      ""
    ],
    "description": "Convolutional Neural Network avec Keras"
  },
  "dl_rnn_with_keras": {
    "prefix": "dl_rnn_with_keras",
    "body": [
      "# 0- Imports",
      "from tensorflow.keras.models import Sequential",
      "from tensorflow.keras.optimizers import Adam",
      "from tensorflow.keras import layers",
      "",
      "# 1- RNN Architecture",
      "model = Sequential()",
      "model.add(layers.SimpleRNN(units=2, activation='tanh', input_shape=(4,3))) # Units : number of memories about features maintained in parallel",
      "# model.add(layers.LSTM(units=5, activation='tanh', input_shape=(4,3)))",
      "model.add(layers.Dense(1, activation=\"linear\"))",
      "",
      "# 2- Compilation",
      "model.compile(loss='mse',",
      "              optimizer=Adam(lr=0.5)) # very high lr so we can converge with such a small dataset",
      "",
      "# 3- Fit",
      "model.fit(X, y, epochs=10000, verbose=0)",
      "",
      "# 4- Predict",
      "model.predict(X) # One prediction per city",
      ""
    ],
    "description": "Recurrent Neural Network avec Keras"
  },
  "dl_nlp_with_keras": {
    "prefix": "dl_nlp_with_keras",
    "body": [
      "import numpy as np",
      "from tensorflow.keras.datasets import imdb",
      "from tensorflow.keras.preprocessing import sequence",
      "from tensorflow.keras.models import Sequential",
      "from tensorflow.keras.layers import Embedding, LSTM, Dense",
      "",
      "# Paramètres",
      "VOCAB_SIZE = 20000  # Nombre de mots uniques à considérer",
      "MAX_SENTENCE_LENGTH = 200         # Couper les textes après ce nombre de mots",
      "EMBED_DIM = 50  # Dimension de l'espace d'embedding",
      "",
      "# Charger les données IMDB",
      "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)",
      "",
      "# Remplissage des séquences pour qu'elles aient la même longueur",
      "x_train = sequence.pad_sequences(x_train, maxlen=max_len)",
      "x_test = sequence.pad_sequences(x_test, maxlen=max_len)",
      "",
      "# Définir l'architecture du modèle",
      "model = Sequential()",
      "model.add(Embedding(input_dim=VOCAB_SIZE,",
      "                    input_length=MAX_SENTENCE_LENGTH,",
      "                    output_dim=EMBED_DIM,",
      "                    mask_zero=True))",
      "model.add(LSTM(64, return_sequences=False))",
      "model.add(Dense(1, activation='sigmoid'))",
      "",
      "# Compiler le modèle",
      "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
      "",
      "# Résumé du modèle",
      "model.summary()",
      "",
      "# Entraîner le modèle",
      "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.2)",
      "",
      "# Évaluer le modèle sur les données de test",
      "score, acc = model.evaluate(x_test, y_test, batch_size=32)",
      "print(f'Score sur les données de test: {score}')",
      "print(f'Accuracy sur les données de test: {acc}')",
      ""
    ],
    "description": "Définir et compiler un modèle LSTM pour le NLP"
  },
  "dl_nlp_tokenize_vocabulary": {
    "prefix": "dl_nlp_tokenize_vocabulary",
    "body": [
      "from tensorflow.keras.preprocessing.text import Tokenizer",
      "",
      "# Exemples de phrases",
      "texts = [",
      "    'I love machine learning. It is awesome.',",
      "    'Deep learning is a subset of machine learning.',",
      "    'Natural language processing is a complex field.'",
      "]",
      "",
      "# Instancier le Tokenizer",
      "tokenizer = Tokenizer(num_words=10000)",
      "",
      "# Ajuster le tokenizer sur les textes",
      "tokenizer.fit_on_texts(texts)",
      "",
      "# Convertir les textes en séquences de tokens",
      "sequences = tokenizer.texts_to_sequences(texts)",
      "",
      "# Uniformiser la longueur des séquences",
      "padded_sequences = pad_sequences(",
      "    sequences,",
      "    maxlen=10,",
      "    padding='post'",
      ")",
      "",
      "# Obtenir le dictionnaire des mots",
      "word_index = tokenizer.word_index",
      "",
      "# Afficher les séquences, les séquences remplies et le dictionnaire des mots",
      "print('Sequences:', sequences)",
      "print('Padded Sequences:', padded_sequences)",
      "print('Word Index:', word_index)",
      ""
    ],
    "description": "Tokenizer le vocabulaire avec Keras"
  },
  "api_create with_fastapi": {
    "prefix": "api_create with_fastapi",
    "body": [
      "from fastapi import FastAPI",
      "from fastapi.middleware.cors import CORSMiddleware",
      "",
      "app = FastAPI()",
      "",
      "# Allowing all middleware is optional, but good practice for dev purposes",
      "app.add_middleware(",
      "    CORSMiddleware,",
      "    allow_origins=[\"*\"],  # Allows all origins",
      "    allow_credentials=True,",
      "    allow_methods=[\"*\"],  # Allows all methods",
      "    allow_headers=[\"*\"],  # Allows all headers",
      ")",
      "",
      "# Définir un endpoint racine",
      "@app.get('/')",
      "def root():",
      "    return {'message': 'Hello World'}",
      "",
      "@app.get('/predict')",
      "def predict(account_amount_added_12_24m,uuid):",
      "",
      "    # Retrieve mes donnees et je les mets dans un dataframe.",
      "    df = pd.DataFrame(",
      "        {",
      "        \"account_amount_added_12_24m\":[account_amount_added_12_24m],",
      "        \"uuid\":[uuid],",
      "        }",
      "    )",
      "",
      "    # Dropper les meme colonnes",
      "    df.drop(columns=['worst_status_active_inv',",
      "                    'account_worst_status_12_24m'], inplace=True)",
      "",
      "    # Load mon model",
      "    new_pipe = pickle.load(open('model.pkl', 'rb'))",
      "",
      "    # Predict de mon df sur le model",
      "    return new_pipe.predict_proba(df)[0][1]",
      "",
      "# uvicorn main:app --reload",
      ""
    ],
    "description": "Création d'un endpoint avec FastAPI"
  },
  "dockerfile": {
    "prefix": "dockerfile",
    "body": [
      "FROM python:3.8-slim",
      "",
      "WORKDIR /app",
      "",
      "COPY requirements.txt requirements.txt",
      "RUN pip install -r requirements.txt",
      "",
      "COPY . .",
      "",
      "CMD ['uvicorn', 'main:app', '--host', '0.0.0.0', '--port', '8000']"
    ],
    "description": "Dockerfile pour créer une image Docker"
  },
  "cloud_run": {
    "prefix": "cloud_run",
    "body": [
      "# Après avoir construit et tagué votre image Docker",
      "docker build -t gcr.io/PROJECT_ID/IMAGE_NAME:latest .",
      "",
      "# Pousser l'image vers Google Container Registry",
      "docker push gcr.io/PROJECT_ID/IMAGE_NAME:latest",
      "",
      "# Déployer sur Google Cloud Run",
      "gcloud run deploy --image gcr.io/PROJECT_ID/IMAGE_NAME:latest --platform managed"
    ],
    "description": "Déploiement sur Google Cloud Run"
  },
  "check_cpu_available": {
    "prefix": "check_cpu_available",
    "body": [
      "# Check CPU's available",
      "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))",
      ""
    ]
  },
  "dl_plot_history_function": {
    "prefix": "dl_plot_history_function",
    "body": [
      "def plot_history(history):",
      "    fig, ax = plt.subplots(1, 2, figsize=(15,5))",
      "    ax[0].set_title('loss')",
      "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")",
      "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")",
      "    ax[1].set_title('accuracy')",
      "    ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train acc\")",
      "    ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation acc\")",
      "    ax[0].legend()",
      "    ax[1].legend()",
      ""
    ]
  },
  "dl_plot_history_compare": {
    "prefix": "dl_plot_history_compare",
    "body": [
      "def plot_compare_history(history, name_history, history_1, name_history_1):",
      "",
      "    fig, ax = plt.subplots(1, 2, figsize=(15,5))",
      "",
      "    ax[0].set_title('loss')",
      "",
      "    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss \" + name_history)",
      "    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss \" + name_history)",
      "",
      "    ax[0].plot(history_1.epoch, history_1.history[\"loss\"], label=\"Train loss \" + name_history_1)",
      "    ax[0].plot(history_1.epoch, history_1.history[\"val_loss\"], label=\"Validation loss \" + name_history_1)",
      "",
      "    ax[1].set_title('Accuracy')",
      "",
      "    ax[1].plot(history.epoch, history.history[\"accuracy\"], label=\"Train Accuracy \" + name_history)",
      "    ax[1].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation Accuracy \" + name_history)",
      "",
      "    ax[1].plot(history_1.epoch, history_1.history[\"accuracy\"], label=\"Train Accuracy \" + name_history_1)",
      "    ax[1].plot(history_1.epoch, history_1.history[\"val_accuracy\"], label=\"Validation Accuracy \" + name_history_1)",
      "",
      "    ax[0].legend()",
      "    ax[1].legend()",
      ""
    ]
  },
  "scatter_like_a_pro_with_plotly": {
    "prefix": "scatter_like_a_pro_with_plotly",
    "body": [
      "import plotly.express as px",
      "",
      "fig = px.scatter(data_frame = sellers[sellers['review_score'] < 4],",
      "    x=\"wait_time\",",
      "    y=\"delay_to_carrier\",",
      "    size=\"sales\",",
      "    color=\"review_score\",",
      "    size_max = 60,",
      "    opacity = 0.5",
      ")",
      "",
      "fig.show()",
      ""
    ]
  },
  "nlp_ner_named_entity_recognition": {
    "prefix": "nlp_ner_named_entity_recognition",
    "description": "Identifies and classifies named entities in text into pre-defined categories",
    "body": [
      "import spacy",
      "from spacy import displacy",
      "nlp = spacy.load(\"en_core_web_sm\")",
      "",
      "text = \"Hi, my name is Aman Kharwal, I work at Statso.io\"",
      "doc = nlp(text)",
      "",
      "displacy.render(doc, style='ent', jupyter=True, options={'ents': ['PERSON', 'ORG'], 'colors': {'PERSON': 'lightblue', 'ORG': 'lime'}})",
      ""
    ]
  },
  "preprocessing_fill_missing_data": {
    "prefix": "preprocessing_fill_missing_data",
    "body": [
      "df.fillna(value='your_value_or_method', inplace=True)",
      ""
    ],
    "description": "Fills missing values in the DataFrame with a specified value or method."
  }
}
